---
title: "Data Preparation Report - Knotweed's Tarping Survey (Dusz et al., 2020)"
author: "François-Marie Martin"
date: "11/27/2020"
output:
   html_document:
     number_sections: yes
     toc: yes
     toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# - Reproductibility
## - System information for my session  

```{r session information}
rm(list=ls())
sessionInfo()
```


  


# - Data visualisation and exploration  
## - Data exploration following Zuur *et al.* (2010)

```{r data import and summary, include=FALSE}
library(jk.dusz.tarping)
zzz_mydata_cleaned <- jk.dusz.tarping::clean_my_data()
```

In order to build valid and meaningful models to explain or predict the success/failure of tarping operations for the control of Japanese knotweed *s.l.*, we need first to learn more about our data. Therefore, we thoroughly examined our variables (e.g. outliers, distribution, interactions, colinearity, variance homogeneity, observation independence) mostly following the protocol presented by Zuur *et al.* (2010) for each of our **5 response variables**: *eff_eradication*, *efficiency*, *latest_reg_edges*, *reg_overlaps*, *reg_tarpedarea*.

As our global dataset contains many variables (*p* = 85), it would be interesting to reduce their number in order to spare time for further exploration and analyses. As such, the first steps of our data exploration for each response variable will aim at reducing the dimensionality of the data. First, we will try and see if the *environmental variables* can be coerced into a synthetic variable (e.g. through a PCA). Then, we will investigate correlations among variables as well as potential colinearity. This should have the twofold advantage of improving our understanding of the data and enabling us to remove colinear and too strongly correlated variables. Finally, we will carry on data exploration in various ways in order to detect outliers, identify distributions, heteroscedasticity, univariate and multivariate relationships...



### - For the response variables related to tarping *efficiency*

The first thing to do here is to prepare the sub-dataset that will be used to model the responses of the *efficiency variables* (i.e. **efficiency**, **eff_eradication** and **high_eff**). To do so, the following steps have been performed:

* We selected the subset of predictors and covariates that will be used to model tarping efficiency;
* We left out the observations that were too recent (installed less than 2 growing seasons ago) and those for which we could not obtain an *efficiency score* (i.e. NA);
* We deleted operations that got prematurely interrupted (e.g. because of vandalism) since there failure results from external causes.
* Finally, we imputed missing values using **Random Forest** and the `missForest` package (Stekhoven & Buehlmann, 2012). 

The table below presents the *out-of-bag (OOB) imputation error* estimates for each variables (computed as Mean Squared Error (MSE) for numeric variables, and Proportion of Falsely Classified (PFC) for categorical variables), compared to the number of imputed values (i.e. number of NAs per variable): OOB values around 1 or > 1 indicate that imputations are not reliable. Accordingly, we will keep an eye on the observations that have unreliable imputations if they become **influential observations** in subsequent models (in which case, we will drop them)! For the meaning of the variables, please refer to the attached **documentation**.

```{r erad import & imput, include=FALSE}
erad <- jk.dusz.tarping::model_datasets(response.var = "efficiency")

erad %>% dplyr::select(-liner_geomem, -agri_geomem,-woven_geotex, -mulching_geotex, -pla_geotex, -other_unknown, -grammage, -age, -thickness, -pierced_tarpinstall, -add_control_type) %>%
  dplyr::filter(eff_eradication != "NA") %>%
  dplyr::filter(tarping_duration >= 2) %>%
  dplyr::filter(xp_id != "54") -> erad # Operation n°54 was destroyed because of vandalism.

### Imputation of missing values using `missForest` (we could also have worked with `MICE`):
erad %>% dplyr::select(-manager_id, -xp_id) %>% as.data.frame() -> erad_mis # missForest only accepts data.frames or matrices (not tibbles). Variables must also be numeric or factors only (no character, date, difftime... classes)!

imput <- missForest::missForest(xmis = erad_mis, verbose = TRUE, variablewise = TRUE)

# To create a small summary table for the OOB errors:
imput_error <- data.frame(cbind(
  sapply(erad_mis, function(y) sum(length(which(is.na(y))))), # Number of imputed values (NAs)
  imput$OOBerror), # Out-of-bag error (OOB) for each variable
  row.names = colnames(erad_mis)) # To get the name of the variables
dplyr::rename(imput_error, nb_imputed_values = 'X1', oob_error = 'X2') -> imput_error
```

```{r erad imput error table, include=TRUE, message=FALSE, comment=NA}
knitr::kable(imput_error)
```

Now, let's take a quick look at our dataset.

```{r erad summary, include=TRUE, message=FALSE, comment=NA}
erad[,3:ncol(erad)] <- imput$ximp # Replacing th original dataset with the imputed one!
summary(erad)

rm(erad_mis, imput, imput_error)
```



#### - Investigation of correlation structures
##### - Multivariate synthesis of environmental variables

We performed a **normed-PCA** on some of our environmental variables for which we do not have strong hypotheses (i.e. *difficulty_access*, *shade*, *forest*, *ruggedness*, *granulometry*) but we kept *slope*, *obstacles*, and *flood* because we suspected they might have a major influence on the success/failure of tarping. 

```{r erad env PCAs, warning=FALSE, fig.align='center', fig.cap="**Figure 1.** Correlation plot of the normed-PCA on environmental variables for the *tarping efficiency* dataset"}
xxx <- dplyr::select(.data = erad, c(difficulty_access, shade, forest, ruggedness, granulometry))

# In order to get optimum results, we imputed missing values using the *Regularised Iterative PCA algorithm* (Josse & Husson, 2013) of the `missMDA` package (Josse & Husson, 2016). 
# But first, we need to estimate how many components are required to correctly impute the missing values:
#nb <- missMDA::estim_ncpPCA(xxx, ncp.max=5) # Can be time consuming. Here, the best result is 0 but we will take 2 as it is (almost) the second best option.
# Then we impute the missing values and extract the imputed dataset:
#res.imput <- missMDA::imputePCA(X = xxx, method = "Regularized", scale = TRUE, ncp = 2)
#xxx <- res.imput$completeObs # Gives the completed dataset!

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a large amount of the variance of my 5 variables (38.2%), I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
erad %>% dplyr::select(-difficulty_access, -shade, -forest, -ruggedness) %>%
  dplyr::rename(coarse_env = "granulometry") -> erad
erad$coarse_env <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.

# To plot side by side:
#gridExtra::grid.arrange(FIG_1, FIG_2, ncol = 2)
```

As the first axis (PC) of our PCA satisfactorily synthesizes a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of the 5 environmental variables, we will use operations' coordinates on this axis to build a synthetic variable to summarize their environment. This new variable will be named **coarse_env** and will thus replace the other 5 variables in the dataset. Operations with *positive values* for this variable are located in sites that tend to be forested (and thus shaded), difficult to access, with uneven ground and a coarse soil texture (and vice-versa).


```{r, eval=FALSE, include=FALSE}
# PEUT ETRE INTERESSANT POUR D'AUTRES MODELES???????? mouais...
# pb <- zzz_mydata_cleaned[, c(59,60,64,66,68)] #PB+REG (2dim = 53%)
# pb <- na.omit(object = pb) # To have no obs. with NAs!
# summary(pb)
# 
# 
# #MCA
# acm <- FactoMineR::MCA(X = pb, ncp = 5,graph = FALSE)
# acm$eig
# plot(acm, label = "var")
# plot(acm, choix = "var")
```

  
##### - Correlation among variables

To investigate potential correlations and colinearity among our variables, we will start by computing a **correlation matrix** of the data. As most of our factors are either ordered factors (i.e. ordinal variables) or binary factors (i.e. boolean), we can use them in a correlation matrix. To do that however, computations will be based on *Spearman*'s rank correlation coefficient.  
To avoid giving too much weight to our own tarping experiments (those performed in Chalon s/ Saone with the French national railway company), we removed them from the data to compute correlations: their influence will be accounted for in further analyses by the use of **mixed-effect models**.

```{r erad corrplot, fig.align='center', fig.cap="**Figure 2.** Correlation matrix displaying *Spearman*'s $\\rho$ for the numeric variables and ordered factors of the *tarping efficiency* dataset. Only significant correlations are displayed (with $\\alpha$ = 0.05)", warning=FALSE}
rm(res.pca, res.imput, xxx, ttt, nb)

# To convert all ordered factors into numeric variables usable in a correlation matrix:
erad %>% dplyr::filter(!manager_id == 1) %>% # We remove our own experiments as they share many features that will strongly influence correlations (it should not be a problem for modelling as we will use mixed-effect models).
  dplyr::select(-manager_id, -xp_id, -goals) %>%
  dplyr::mutate_if(.predicate = is.factor, .funs = as.numfactor) -> num.erad # as.numfactor() is my own custom function (see '01_data_cleaning.R').



# To compute the correlation matrix:
res.cor.erad <- round(stats::cor(num.erad, use = "complete.obs", method = "spearman"), 2)
# To compute a matrix of correlation p-values:
res.pcor.erad <- ggcorrplot::cor_pmat(x = num.erad, method = "spearman")

ggcorrplot::ggcorrplot(res.cor.erad, type = "upper",
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"), p.mat = res.pcor.erad, insig = "blank")

rm(num.erad, res.cor.erad, res.pcor.erad)
```

Many interesting patterns can be observed in this correlation matrix. Among them, we can see that:

* One or several of the *efficiency* variables seem to be mildly positively correlated with *distance*, and *fully_tarped* and negatively with *obstacles*, *add_control*, and *pb_durability*. 
* *geomem* is extremely negatively correlated with *geotex*, which is logical as one is the opposite of the other. 
* *trench_depth* seems to be strongly positively correlated with *tarpfix_multimethod*, which is logical as trenches are strong fixation method. 
* *pb_fixation* seems to be strongly positively correlated with *stand_surface* and, to a lesser extent, with *tarpfix_pierced* and *repairs*.
* *obstacles* is positively correlated with *slope* and *coarse_env* (which are also positively correlated), as well as with *pb_durability*, and negatively with *distance*: all these correlations seem quite straightforward. 
* Interestingly, *flood* is positively correlated with *geotex*, *tarpfix_pierced*, *add_control* and *pb_fixation*.

It is worth reminding that accounting for multivariate relationships, interactions and the effect of our tarping operations (dropped for the current correlation matrix) will certainly give very different patterns.


   
##### - Multicollinearity

As multicollinearity is one of the biggest issues for inference modelling, we need to assess which explanatory variables and covariates are collinear and will possibly become problematic for modelling. To do so, our intention was to compute *Variance Inflation Factors* (VIF) for each variable. However, as several variables are categorical and as we will not be working with models fitted with ordinary least squares, we will rather use *Generalized VIF* (Fox & Monette, 1992).

```{r erad init GVIF}
modvif <- stats::glm((efficiency/10)~latitude+longitude+elevation+freq_monitoring+slope+coarse_env+obstacles+flood+geomem+geotex+weedsp_geotex+maxveg+uprootexcav+stand_surface+fully_tarped+distance+tarping_duration+stripsoverlap_ok+tarpfix_multimethod+tarpfix_pierced+sedicover_height+trench_depth+plantation+repairs+add_control+degradation+pb_fixation+pb_durability, data = erad, family = quasibinomial)
#summary(modvif)

knitr::kable(car::vif(modvif))
```

As one could expect, there is evidence for **multicollinearity** among our 26 potential explanatory variables. However, strong collinearity among variables is only a problem if these variables are included together in the same model, hence we should reevaluate collinearity more precisely when actual working models will be built. Here, we will simply identify which variables will be generally too problematic across likely models and, possibly, drop them (we will try to lower GVIF values as much as possible, considering that, even GVIF of 2 could still be problematic if ecological signals are weak; cf. Zuur *et al.*, 2010):

* The variables *geomem* and *geotex* (and *weedsp_geotex*) are, without a surprise, problematic as they contain the same information. Therefore, we decide to only keep *geomem*.
* The *plantation* variable also seems highly problematic. Yet, as we have a strong interest in that variable, we choose to drop it **only temporarily**, only to detect other problematic variables. A particular attention to multicollinearity should be maintained when this variable will be included within models.
* Since our dataset contains sites on both sides of the Atlantic and since we do not have strong hypotheses regarding the effects of *longitude* (because our sites are located well within the longitudinal range of knotweeds), we decide to drop it.
* Since *degradation* contains most of the information contained in *pb_fixation* and *pb_durability*, we decide to drop it to reduce the number of variables. 

After iteratively dropping these collinear variables and re-assessing GVIF values, we obtain a subset of potential explanatory variables with GVIF values globally < 3 and GVIF values will likely be even lower in later stages as models will not include so many variables at the same time. Naturally, we will also bear in mind the potential effects of the dropped variables in the discussion.

```{r erad final GVIF}
modvif <- stats::glm((efficiency/10)~latitude+elevation+freq_monitoring+slope+coarse_env+obstacles+flood+geomem+maxveg+uprootexcav+stand_surface+fully_tarped+distance+tarping_duration+stripsoverlap_ok+tarpfix_multimethod+tarpfix_pierced+sedicover_height+trench_depth+repairs+add_control+pb_fixation+pb_durability, data = erad, family = quasibinomial)
#summary(modvif)

knitr::kable(car::vif(modvif))

erad %>% dplyr::select(-geotex, -weedsp_geotex, -longitude, -degradation) -> erad

rm(modvif)
```


    
#### - Distributions and dispersion parameters
##### - Looking for outliers

We will look for potential outliers in the data using **boxplots** and **Cleveland dotplots** on our quantitative variables. 

```{r erad boxplot outliers, fig.align='center', fig.cap="**Figure 3.** Boxplots for all quantitative variables of the *eradication efficiency* dataset", warning=FALSE}
### IMPORTANT NOTE: as ggplot2 requires data in the long format to be able to do multi-panels plots (known as facets), we need to transform our data using the pivot_longer() function of {tidyr}. 
# To keep only numeric columns: num.erad <- erad[, sapply(erad, is.numeric)]; or we can do:

# With ggplot2, it is time-consuming!
#erad %>%
  #purrr::keep(is.numeric) %>% # Keep only numeric columns
  #tidyr::pivot_longer(cols = tidyselect::everything()) %>% # Pivot every columns
  #ggplot2::ggplot(ggplot2::aes(x = name, y = value)) +
  #ggplot2::geom_boxplot(fill = "lightsalmon") +
  #ggplot2::theme_minimal() -> ppp
#ppp + ggplot2::facet_wrap( ~ name, scales = "free")

#RColorBrewer::display.brewer.all() To display all the colors in {RColorBrewer}

jk.dusz.tarping::uni.boxplots(dataset = erad)
```

```{r erad dotplot, fig.align='center', fig.cap="**Figure 4.** Cleveland dotplots for all quantitative variables of the *eradication efficiency* dataset"}
jk.dusz.tarping::uni.dotplots(dataset = erad)
```

By the look of these graphs, it is clear that we have many *extreme values* (possible outliers). Yet, extreme values are not necessarily **true outliers**:

* The variables *latitude*, *slope* and *coarse_env*  look fine.
* The variables *efficiency*, *obstacles*, *flood*, *distance*, *tarping_duration*, and *trench_depth* may require mild transformations to be normalized but do not seem to contain too extreme values.
* The variables *elevation*, *freq_monitoring*, and *sedicover_height* may require important transformations to be normalized. It would be interesting to see if their extreme values are too far from what could be expected from a Normal or a Poisson distribution (see below).
* The *stand_surface* variable contains a problematic patterns/outliers.


```{r erad simu.distrib, include=FALSE, eval=FALSE}
simu.var <- dplyr::select(.data = erad, elevation, freq_monitoring, stand_surface, distance, sedicover_height, trench_depth, tarping_duration)

jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "log-normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "poisson")
rm(simu.var)

```

After analysing random samples drawn from log-normal distributions (based on the parameters of our potentially problematic variables), we have good reason to believe that only *stand_surface* should be problematic. Consequently, we choose to delete the spotted outlier within *stand_surface* (i.e. the *stand_surface* of ca. 5000 m^2^).    
The other variables do not have distribution that a transformation could not fix. We will see later on whether we actually need such transformation (e.g log) or not. 


```{r erad update, include=FALSE}
erad <- erad[-(which.max(erad$stand_surface)),]
```


   
##### - Normality

We already got a pretty good idea of the distribution of our variables and not all statistical methods involve a Normality assumption (for instance, multiple linear regressions assume that residuals are normally distributed, not the variable themselves). Still, it's interesting and often useful to have a precise view of variables distributions, even for predictors and covariates. 

```{r erad histo, fig.align='center', fig.cap="**Figure 5.** Histograms for all quantitative variables of the *eradication efficiency* dataset"}
uni.histograms(dataset = erad)
```





#### - Bivariate relationships
##### - Relationships among variables

The next step in our data exploration is to plot the response variables against each predictors and covariates in order to assess the **linearity** and **homoscedasticity** assumptions as well as identify potential **interactions** and **multivariate outliers**. It would be interesting to plot all variables against each other, but we have too many potential predictors and covariates for that.

```{r erad bivariate plot, fig.align='center', fig.cap="**Figure 6.** Multivariate representation of the *efficiency score* against each potential explanatory variable with a focus on whether knotweed stands were fully tarped or not", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
### To plot all variables against each other, we can use pairs() or ggpairs():
# ggplot2::theme_set(ggplot2::theme_bw())
# erad %>% dplyr::select(-xp_id, -goals) %>% GGally::ggpairs()

### To plot a response variable against all predictors/covariates, we will have to do use a for loop or the facet_wrap function of ggplot (or equivalent).
# For the "efficiency" variable (bounded continuous variable):
erad %>% dplyr::select(-manager_id, -xp_id, -goals, -eff_eradication, -high_eff) %>%
  dplyr::mutate_if(is.factor, as.numfactor) %>% 
  tidyr::pivot_longer(-c(efficiency, fully_tarped), # Means that all columns of `mydata` need to be reshaped except "efficiency" & "fully_tarped" (works as well with the form `!efficiency`, and we can place it somewhere else: actually, it is the `cols =` argument)! We have to work in the long-format because that's how the tidyverser and thus ggplot2 love to work...
                               names_to = "Predictor", # Gives the name of the new grouping variable while "values_to" gives the name of the variable that will be created from the data stored in the cell value.
                               values_to = "Value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = Value, y = efficiency)) +
  ggplot2::facet_wrap(~Predictor, scales = "free_x", strip.position = "bottom") +
  ggplot2::geom_point(size = 1.5, alpha = 0.8, ggplot2::aes(col = as.factor(fully_tarped))) +
  ggplot2::scale_y_continuous(limits=c(0,10)) +
  ggplot2::geom_smooth(method = "lm", na.rm = TRUE, size = 1, 
                       ggplot2::aes(col = as.factor(fully_tarped))) + # If you want to affect something related to the input dataframe (such as here, where we want a smoother for each value of "fully_tarped"), you have to put the command in an aes() function!
# If a "loess" smoother is used, sometimes the smoother line won't be plotted, I don't know why.
  ggplot2::scale_colour_manual(values = c("plum1", "palevioletred4")) +
  ggplot2::labs(x = "Predictor value", y = "Efficiency score", col = "Fully tarped") +
  ggthemes::theme_hc(style = "darkunica") +
  ggplot2::theme(panel.border = ggplot2::element_blank(),
          legend.key = ggplot2::element_blank(),
          strip.background = ggplot2::element_blank(),
          strip.text = ggplot2::element_text(colour = "gray60", size = 10),
          strip.placement = "outside",
          axis.line.x = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          axis.line.y = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          legend.position = "right")
```

Various interesting observations can be made from these scatterplots:

* First, heteroscedasticity will perhaps not be surprising in our models as the variances of many variables do not seem to be homogeneous. On the other hand, we do not see clear multivariate outliers.
* Second, the *efficiency* score seems to increase with increasing *distance*. This may be evidence in support to one of our working hypotheses stating that successful tarping operations cover areas well beyond the visible limits of knotweed stands. Conversely, the *efficiency* score seems to decrease with increasing presence of *obstacles* and with *pb_durability*.
* Third, surprisingly, the *efficiency* score also seems to increase with increasing *elevation*. If this is not due to measurement error, then it could be a sign of stronger abiotic constraints on knotweeds. *Additional control* similarly seems to improve *efficiency*.
* Fourth, there may be signs of interesting interactions between the *fully tarped* variable and *coarse_env*, *pb_durability*, *slope*, *stand_surface*, *tarpfix_pierced*, *tarping_duration*, *trench_depth* and *uprootexcav*, although caution is required as variances are quite high. Furthermore, if interactions with *stand_surface* or *pb_durability* are relatively straightforward, the other ones are trickier do grasp and could simply be spurious.

To further investigate multivariate relationship, we will create another set of scatterplots (but we do not intend on investigating all possible combinations between variables).

```{r erad bivariate plot2, fig.align='center', fig.cap="**Figure 6bis.** Multivariate representation of the *efficiency score* against each potential explanatory variable with a focus on whether knotweed stands were fully tarped or not", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
erad %>% dplyr::select(-manager_id, -xp_id, -goals, -eff_eradication, -high_eff) %>%
  dplyr::mutate_if(is.factor, as.numfactor) %>% 
  tidyr::pivot_longer(-c(efficiency, plantation), 
                               names_to = "Predictor", 
                               values_to = "Value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = Value, y = efficiency)) +
  ggplot2::facet_wrap(~Predictor, scales = "free_x", strip.position = "bottom") +
  ggplot2::geom_point(size = 1.5, alpha = 0.8, ggplot2::aes(col = as.factor(plantation))) +
  ggplot2::scale_y_continuous(limits=c(0,10.5)) +
  ggplot2::geom_smooth(method = "lm", na.rm = TRUE, size = 1, 
                       ggplot2::aes(col = as.factor(plantation))) + 
  ggplot2::scale_colour_manual(values = c("goldenrod3", "coral", "darkred")) +
  ggplot2::labs(x = "Predictor value", y = "Efficiency score", col = "Plantation") +
  ggthemes::theme_hc(style = "darkunica") +
  ggplot2::theme(panel.border = ggplot2::element_blank(),
          legend.key = ggplot2::element_blank(),
          strip.background = ggplot2::element_blank(),
          strip.text = ggplot2::element_text(colour = "gray60", size = 10),
          strip.placement = "outside",
          axis.line.x = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          axis.line.y = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          legend.position = "right")
```

We can see from this new panel of plots that the *efficiency* score seems to increase when knotweeds are *fully tarped*, as we hypothesized. 

```{r erad export, include=FALSE}
### Finally, I export the dataset to be used for modelling in my "mydata" folder:
readr::write_csv(x = erad, file = here::here("mydata", "erad.csv"), append = FALSE)
# A problem with this method is that once exported, R will lose track of the variable types (class). Therefore, if we open (read) this .csv file with R, it will specify columns as best it can, that is as double for numeric columns and as character for columns containing letters. For instance, it will import the first column of `erad` (i.e. "manager_id") as a numeric (double) variable when it should be a factor.
# A not so ideal solution is to use the `col_types` argument of the readr::read_csv() function to manually specify variable types during import. Yet, we will not be able to specify that "plantation" is an ordered factor and we will have to do it manually afterward. 
```





### - For the response variables related to tarping *lreg_edges*

The first thing to do here is to prepare the sub-dataset that will be used to model the responses of the *lreg_edges* variable. To do so, the following steps have been performed:

* We selected the subset of predictors and covariates that will be used to model *lreg_edges*;
* We left out the observations for which we could not obtain information on regrowths (i.e. NA);
* We deleted operations that got prematurely interrupted (e.g. because of vandalism) since there regrowths may be the result of external causes.
* Finally, we imputed missing values using **Random Forest** and the `missForest` package (Stekhoven & Buehlmann, 2012). 

The table below presents the *out-of-bag (OOB) imputation error* estimates for each variables (computed as Mean Squared Error (MSE) for numeric variables, and Proportion of Falsely Classified (PFC) for categorical variables), compared to the number of imputed values (i.e. number of NAs per variable): OOB values around 1 or > 1 indicate that imputations are not reliable. Accordingly, we will keep an eye on the observations that have unreliable imputations if they become **influential observations** in subsequent models (in which case, we will drop them)! For the meaning of the variables, please refer to the attached **documentation**.

```{r redges import & imput, include=FALSE}
redges <- jk.dusz.tarping::model_datasets(response.var = "edges")

redges %>% dplyr::select(-liner_geomem, -agri_geomem,-woven_geotex, -mulching_geotex, -pla_geotex, -other_unknown, -grammage, -age, -thickness, -pierced_tarpinstall, -add_control_type) %>%
  dplyr::filter(eff_redgesication != "NA") %>%
  dplyr::filter(tarping_duration >= 2) %>%
  dplyr::filter(xp_id != "54") -> redges # Operation n°54 was destroyed because of vandalism.

### Imputation of missing values using `missForest` (we could also have worked with `MICE`):
redges %>% dplyr::select(-manager_id, -xp_id) %>% as.data.frame() -> redges_mis # missForest only accepts data.frames or matrices (not tibbles). Variables must also be numeric or factors only (no character, date, difftime... classes)!

imput <- missForest::missForest(xmis = redges_mis, verbose = TRUE, variablewise = TRUE)

# To create a small summary table for the OOB errors:
imput_error <- data.frame(cbind(
  sapply(redges_mis, function(y) sum(length(which(is.na(y))))), # Number of imputed values (NAs)
  imput$OOBerror), # Out-of-bag error (OOB) for each variable
  row.names = colnames(redges_mis)) # To get the name of the variables
dplyr::rename(imput_error, nb_imputed_values = 'X1', oob_error = 'X2') -> imput_error
```

```{r redges imput error table, include=TRUE, message=FALSE, comment=NA}
knitr::kable(imput_error)
```

Now, let's take a quick look at our dataset.

```{r redges summary, include=TRUE, message=FALSE, comment=NA}
redges[,3:ncol(redges)] <- imput$ximp # Replacing th original dataset with the imputed one!
summary(redges)

rm(redges_mis, imput, imput_error)
```



#### - Investigation of correlation structures
##### - Multivariate synthesis of environmental variables

We performed a **normed-PCA** on some of our environmental variables for which we do not have strong hypotheses (i.e. *difficulty_access*, *shade*, *forest*, *ruggedness*, *granulometry*) but we kept *slope*, *obstacles*, and *flood* because we suspected they might have a major influence on the success/failure of tarping. 

```{r redges env PCAs, warning=FALSE, fig.align='center', fig.cap="**Figure 1.** Correlation plot of the normed-PCA on environmental variables for the *tarping efficiency* dataset"}
xxx <- dplyr::select(.data = redges, c(difficulty_access, shade, forest, ruggedness, granulometry))

# In order to get optimum results, we imputed missing values using the *Regularised Iterative PCA algorithm* (Josse & Husson, 2013) of the `missMDA` package (Josse & Husson, 2016). 
# But first, we need to estimate how many components are required to correctly impute the missing values:
#nb <- missMDA::estim_ncpPCA(xxx, ncp.max=5) # Can be time consuming. Here, the best result is 0 but we will take 2 as it is (almost) the second best option.
# Then we impute the missing values and extract the imputed dataset:
#res.imput <- missMDA::imputePCA(X = xxx, method = "Regularized", scale = TRUE, ncp = 2)
#xxx <- res.imput$completeObs # Gives the completed dataset!

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a large amount of the variance of my 5 variables (38.2%), I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
redges %>% dplyr::select(-difficulty_access, -shade, -forest, -ruggedness) %>%
  dplyr::rename(coarse_env = "granulometry") -> redges
redges$coarse_env <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.

# To plot side by side:
#gridExtra::grid.arrange(FIG_1, FIG_2, ncol = 2)
```

As the first axis (PC) of our PCA satisfactorily synthesizes a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of the 5 environmental variables, we will use operations' coordinates on this axis to build a synthetic variable to summarize their environment. This new variable will be named **coarse_env** and will thus replace the other 5 variables in the dataset. Operations with *positive values* for this variable are located in sites that tend to be forested (and thus shaded), difficult to access, with uneven ground and a coarse soil texture (and vice-versa).


```{r, eval=FALSE, include=FALSE}
# PEUT ETRE INTERESSANT POUR D'AUTRES MODELES???????? mouais...
# pb <- zzz_mydata_cleaned[, c(59,60,64,66,68)] #PB+REG (2dim = 53%)
# pb <- na.omit(object = pb) # To have no obs. with NAs!
# summary(pb)
# 
# 
# #MCA
# acm <- FactoMineR::MCA(X = pb, ncp = 5,graph = FALSE)
# acm$eig
# plot(acm, label = "var")
# plot(acm, choix = "var")
```

  
##### - Correlation among variables

To investigate potential correlations and colinearity among our variables, we will start by computing a **correlation matrix** of the data. As most of our factors are either ordered factors (i.e. ordinal variables) or binary factors (i.e. boolean), we can use them in a correlation matrix. To do that however, computations will be based on *Spearman*'s rank correlation coefficient.  
To avoid giving too much weight to our own tarping experiments (those performed in Chalon s/ Saone with the French national railway company), we removed them from the data to compute correlations: their influence will be accounted for in further analyses by the use of **mixed-effect models**.

```{r redges corrplot, fig.align='center', fig.cap="**Figure 2.** Correlation matrix displaying *Spearman*'s $\\rho$ for the numeric variables and ordered factors of the *tarping efficiency* dataset. Only significant correlations are displayed (with $\\alpha$ = 0.05)", warning=FALSE}
rm(res.pca, res.imput, xxx, ttt, nb)

# To convert all ordered factors into numeric variables usable in a correlation matrix:
redges %>% dplyr::filter(!manager_id == 1) %>% # We remove our own experiments as they share many features that will strongly influence correlations (it should not be a problem for modelling as we will use mixed-effect models).
  dplyr::select(-manager_id, -xp_id, -goals) %>%
  dplyr::mutate_if(.predicate = is.factor, .funs = as.numfactor) -> num.redges # as.numfactor() is my own custom function (see '01_data_cleaning.R').



# To compute the correlation matrix:
res.cor.redges <- round(stats::cor(num.redges, use = "complete.obs", method = "spearman"), 2)
# To compute a matrix of correlation p-values:
res.pcor.redges <- ggcorrplot::cor_pmat(x = num.redges, method = "spearman")

ggcorrplot::ggcorrplot(res.cor.redges, type = "upper",
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"), p.mat = res.pcor.redges, insig = "blank")

rm(num.redges, res.cor.redges, res.pcor.redges)
```

Many interesting patterns can be observed in this correlation matrix. Among them, we can see that:

* One or several of the *efficiency* variables seem to be mildly positively correlated with *distance*, and *fully_tarped* and negatively with *obstacles*, *add_control*, and *pb_durability*. 
* *geomem* is extremely negatively correlated with *geotex*, which is logical as one is the opposite of the other. 
* *trench_depth* seems to be strongly positively correlated with *tarpfix_multimethod*, which is logical as trenches are strong fixation method. 
* *pb_fixation* seems to be strongly positively correlated with *stand_surface* and, to a lesser extent, with *tarpfix_pierced* and *repairs*.
* *obstacles* is positively correlated with *slope* and *coarse_env* (which are also positively correlated), as well as with *pb_durability*, and negatively with *distance*: all these correlations seem quite straightforward. 
* Interestingly, *flood* is positively correlated with *geotex*, *tarpfix_pierced*, *add_control* and *pb_fixation*.

It is worth reminding that accounting for multivariate relationships, interactions and the effect of our tarping operations (dropped for the current correlation matrix) will certainly give very different patterns.


   
##### - Multicollinearity

As multicollinearity is one of the biggest issues for inference modelling, we need to assess which explanatory variables and covariates are collinear and will possibly become problematic for modelling. To do so, our intention was to compute *Variance Inflation Factors* (VIF) for each variable. However, as several variables are categorical and as we will not be working with models fitted with ordinary least squares, we will rather use *Generalized VIF* (Fox & Monette, 1992).

```{r redges init GVIF}
modvif <- stats::glm((efficiency/10)~latitude+longitude+elevation+freq_monitoring+slope+coarse_env+obstacles+flood+geomem+geotex+weedsp_geotex+maxveg+uprootexcav+stand_surface+fully_tarped+distance+tarping_duration+stripsoverlap_ok+tarpfix_multimethod+tarpfix_pierced+sedicover_height+trench_depth+plantation+repairs+add_control+degradation+pb_fixation+pb_durability, data = redges, family = quasibinomial)
#summary(modvif)

knitr::kable(car::vif(modvif))
```

As one could expect, there is evidence for **multicollinearity** among our 26 potential explanatory variables. However, strong collinearity among variables is only a problem if these variables are included together in the same model, hence we should reevaluate collinearity more precisely when actual working models will be built. Here, we will simply identify which variables will be generally too problematic across likely models and, possibly, drop them (we will try to lower GVIF values as much as possible, considering that, even GVIF of 2 could still be problematic if ecological signals are weak; cf. Zuur *et al.*, 2010):

* The variables *geomem* and *geotex* (and *weedsp_geotex*) are, without a surprise, problematic as they contain the same information. Therefore, we decide to only keep *geomem*.
* The *plantation* variable also seems highly problematic. Yet, as we have a strong interest in that variable, we choose to drop it **only temporarily**, only to detect other problematic variables. A particular attention to multicollinearity should be maintained when this variable will be included within models.
* Since our dataset contains sites on both sides of the Atlantic and since we do not have strong hypotheses regarding the effects of *longitude* (because our sites are located well within the longitudinal range of knotweeds), we decide to drop it.
* Since *degradation* contains most of the information contained in *pb_fixation* and *pb_durability*, we decide to drop it to reduce the number of variables. 

After iteratively dropping these collinear variables and re-assessing GVIF values, we obtain a subset of potential explanatory variables with GVIF values globally < 3 and GVIF values will likely be even lower in later stages as models will not include so many variables at the same time. Naturally, we will also bear in mind the potential effects of the dropped variables in the discussion.

```{r redges final GVIF}
modvif <- stats::glm((efficiency/10)~latitude+elevation+freq_monitoring+slope+coarse_env+obstacles+flood+geomem+maxveg+uprootexcav+stand_surface+fully_tarped+distance+tarping_duration+stripsoverlap_ok+tarpfix_multimethod+tarpfix_pierced+sedicover_height+trench_depth+repairs+add_control+pb_fixation+pb_durability, data = redges, family = quasibinomial)
#summary(modvif)

knitr::kable(car::vif(modvif))

redges %>% dplyr::select(-geotex, -weedsp_geotex, -longitude, -degradation) -> redges

rm(modvif)
```


    
#### - Distributions and dispersion parameters
##### - Looking for outliers

We will look for potential outliers in the data using **boxplots** and **Cleveland dotplots** on our quantitative variables. 

```{r redges boxplot outliers, fig.align='center', fig.cap="**Figure 3.** Boxplots for all quantitative variables of the *redgesication efficiency* dataset", warning=FALSE}
### IMPORTANT NOTE: as ggplot2 requires data in the long format to be able to do multi-panels plots (known as facets), we need to transform our data using the pivot_longer() function of {tidyr}. 
# To keep only numeric columns: num.redges <- redges[, sapply(redges, is.numeric)]; or we can do:

# With ggplot2, it is time-consuming!
#redges %>%
  #purrr::keep(is.numeric) %>% # Keep only numeric columns
  #tidyr::pivot_longer(cols = tidyselect::everything()) %>% # Pivot every columns
  #ggplot2::ggplot(ggplot2::aes(x = name, y = value)) +
  #ggplot2::geom_boxplot(fill = "lightsalmon") +
  #ggplot2::theme_minimal() -> ppp
#ppp + ggplot2::facet_wrap( ~ name, scales = "free")

#RColorBrewer::display.brewer.all() To display all the colors in {RColorBrewer}

jk.dusz.tarping::uni.boxplots(dataset = redges)
```

```{r redges dotplot, fig.align='center', fig.cap="**Figure 4.** Cleveland dotplots for all quantitative variables of the *redgesication efficiency* dataset"}
jk.dusz.tarping::uni.dotplots(dataset = redges)
```

By the look of these graphs, it is clear that we have many *extreme values* (possible outliers). Yet, extreme values are not necessarily **true outliers**:

* The variables *latitude*, *slope* and *coarse_env*  look fine.
* The variables *efficiency*, *obstacles*, *flood*, *distance*, *tarping_duration*, and *trench_depth* may require mild transformations to be normalized but do not seem to contain too extreme values.
* The variables *elevation*, *freq_monitoring*, and *sedicover_height* may require important transformations to be normalized. It would be interesting to see if their extreme values are too far from what could be expected from a Normal or a Poisson distribution (see below).
* The *stand_surface* variable contains a problematic patterns/outliers.


```{r redges simu.distrib, include=FALSE, eval=FALSE}
simu.var <- dplyr::select(.data = redges, elevation, freq_monitoring, stand_surface, distance, sedicover_height, trench_depth, tarping_duration)

jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "log-normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "poisson")
rm(simu.var)

```

After analysing random samples drawn from log-normal distributions (based on the parameters of our potentially problematic variables), we have good reason to believe that only *stand_surface* should be problematic. Consequently, we choose to delete the spotted outlier within *stand_surface* (i.e. the *stand_surface* of ca. 5000 m^2^).    
The other variables do not have distribution that a transformation could not fix. We will see later on whether we actually need such transformation (e.g log) or not. 


```{r redges update, include=FALSE}
redges <- redges[-(which.max(redges$stand_surface)),]
```


   
##### - Normality

We already got a pretty good idea of the distribution of our variables and not all statistical methods involve a Normality assumption (for instance, multiple linear regressions assume that residuals are normally distributed, not the variable themselves). Still, it's interesting and often useful to have a precise view of variables distributions, even for predictors and covariates. 

```{r redges histo, fig.align='center', fig.cap="**Figure 5.** Histograms for all quantitative variables of the *redgesication efficiency* dataset"}
uni.histograms(dataset = redges)
```





#### - Bivariate relationships
##### - Relationships among variables

The next step in our data exploration is to plot the response variables against each predictors and covariates in order to assess the **linearity** and **homoscedasticity** assumptions as well as identify potential **interactions** and **multivariate outliers**. It would be interesting to plot all variables against each other, but we have too many potential predictors and covariates for that.

```{r redges bivariate plot, fig.align='center', fig.cap="**Figure 6.** Multivariate representation of the *efficiency score* against each potential explanatory variable with a focus on whether knotweed stands were fully tarped or not", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
### To plot all variables against each other, we can use pairs() or ggpairs():
# ggplot2::theme_set(ggplot2::theme_bw())
# redges %>% dplyr::select(-xp_id, -goals) %>% GGally::ggpairs()

### To plot a response variable against all predictors/covariates, we will have to do use a for loop or the facet_wrap function of ggplot (or equivalent).
# For the "efficiency" variable (bounded continuous variable):
redges %>% dplyr::select(-manager_id, -xp_id, -goals, -eff_redgesication, -high_eff) %>%
  dplyr::mutate_if(is.factor, as.numfactor) %>% 
  tidyr::pivot_longer(-c(efficiency, fully_tarped), # Means that all columns of `mydata` need to be reshaped except "efficiency" & "fully_tarped" (works as well with the form `!efficiency`, and we can place it somewhere else: actually, it is the `cols =` argument)! We have to work in the long-format because that's how the tidyverser and thus ggplot2 love to work...
                               names_to = "Predictor", # Gives the name of the new grouping variable while "values_to" gives the name of the variable that will be created from the data stored in the cell value.
                               values_to = "Value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = Value, y = efficiency)) +
  ggplot2::facet_wrap(~Predictor, scales = "free_x", strip.position = "bottom") +
  ggplot2::geom_point(size = 1.5, alpha = 0.8, ggplot2::aes(col = as.factor(fully_tarped))) +
  ggplot2::scale_y_continuous(limits=c(0,10)) +
  ggplot2::geom_smooth(method = "lm", na.rm = TRUE, size = 1, 
                       ggplot2::aes(col = as.factor(fully_tarped))) + # If you want to affect something related to the input dataframe (such as here, where we want a smoother for each value of "fully_tarped"), you have to put the command in an aes() function!
# If a "loess" smoother is used, sometimes the smoother line won't be plotted, I don't know why.
  ggplot2::scale_colour_manual(values = c("plum1", "palevioletred4")) +
  ggplot2::labs(x = "Predictor value", y = "Efficiency score", col = "Fully tarped") +
  ggthemes::theme_hc(style = "darkunica") +
  ggplot2::theme(panel.border = ggplot2::element_blank(),
          legend.key = ggplot2::element_blank(),
          strip.background = ggplot2::element_blank(),
          strip.text = ggplot2::element_text(colour = "gray60", size = 10),
          strip.placement = "outside",
          axis.line.x = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          axis.line.y = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          legend.position = "right")
```

Various interesting observations can be made from these scatterplots:

* First, heteroscedasticity will perhaps not be surprising in our models as the variances of many variables do not seem to be homogeneous. On the other hand, we do not see clear multivariate outliers.
* Second, the *efficiency* score seems to increase with increasing *distance*. This may be evidence in support to one of our working hypotheses stating that successful tarping operations cover areas well beyond the visible limits of knotweed stands. Conversely, the *efficiency* score seems to decrease with increasing presence of *obstacles* and with *pb_durability*.
* Third, surprisingly, the *efficiency* score also seems to increase with increasing *elevation*. If this is not due to measurement error, then it could be a sign of stronger abiotic constraints on knotweeds. *Additional control* similarly seems to improve *efficiency*.
* Fourth, there may be signs of interesting interactions between the *fully tarped* variable and *coarse_env*, *pb_durability*, *slope*, *stand_surface*, *tarpfix_pierced*, *tarping_duration*, *trench_depth* and *uprootexcav*, although caution is required as variances are quite high. Furthermore, if interactions with *stand_surface* or *pb_durability* are relatively straightforward, the other ones are trickier do grasp and could simply be spurious.

To further investigate multivariate relationship, we will create another set of scatterplots (but we do not intend on investigating all possible combinations between variables).

```{r redges bivariate plot2, fig.align='center', fig.cap="**Figure 6bis.** Multivariate representation of the *efficiency score* against each potential explanatory variable with a focus on whether knotweed stands were fully tarped or not", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
redges %>% dplyr::select(-manager_id, -xp_id, -goals, -eff_redgesication, -high_eff) %>%
  dplyr::mutate_if(is.factor, as.numfactor) %>% 
  tidyr::pivot_longer(-c(efficiency, plantation), 
                               names_to = "Predictor", 
                               values_to = "Value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = Value, y = efficiency)) +
  ggplot2::facet_wrap(~Predictor, scales = "free_x", strip.position = "bottom") +
  ggplot2::geom_point(size = 1.5, alpha = 0.8, ggplot2::aes(col = as.factor(plantation))) +
  ggplot2::scale_y_continuous(limits=c(0,10.5)) +
  ggplot2::geom_smooth(method = "lm", na.rm = TRUE, size = 1, 
                       ggplot2::aes(col = as.factor(plantation))) + 
  ggplot2::scale_colour_manual(values = c("goldenrod3", "coral", "darkred")) +
  ggplot2::labs(x = "Predictor value", y = "Efficiency score", col = "Plantation") +
  ggthemes::theme_hc(style = "darkunica") +
  ggplot2::theme(panel.border = ggplot2::element_blank(),
          legend.key = ggplot2::element_blank(),
          strip.background = ggplot2::element_blank(),
          strip.text = ggplot2::element_text(colour = "gray60", size = 10),
          strip.placement = "outside",
          axis.line.x = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          axis.line.y = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          legend.position = "right")
```

We can see from this new panel of plots that the *efficiency* score seems to increase when knotweeds are *fully tarped*, as we hypothesized. 

```{r redges export, include=FALSE}
### Finally, I export the dataset to be used for modelling in my "mydata" folder:
readr::write_csv(x = redges, file = here::here("mydata", "redges.csv"), append = FALSE)
# A problem with this method is that once exported, R will lose track of the variable types (class). Therefore, if we open (read) this .csv file with R, it will specify columns as best it can, that is as double for numeric columns and as character for columns containing letters. For instance, it will import the first column of `redges` (i.e. "manager_id") as a numeric (double) variable when it should be a factor.
# A not so ideal solution is to use the `col_types` argument of the readr::read_csv() function to manually specify variable types during import. Yet, we will not be able to specify that "plantation" is an ordered factor and we will have to do it manually afterward. 
```



## Next steps for preparation

2) Write other preparation chapters! Terminate excel_model and send it...
3) Set drake?
3) Transformations and standardization (for scaling, especially important when I use interactions terms; cf. "standardization" post on CV in my MacBook favorites)

