---
title: "Data Preparation Report - Knotweed's Tarping Survey (Dusz et al., 2020)"
author: "Fran√ßois-Marie Martin"
date: "11/27/2020"
output:
   html_document:
     number_sections: yes
     toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# - Reproductibility
## - System information for my session  

```{r session information}
rm(list=ls())
sessionInfo()
```
  
  



# - Data visualisation and exploration  
## - Data summary

First, let's take a quick look at our dataset. For the meaning of the variables, please refer to the attached **documentation**.

```{r data import and summary, message=FALSE, comment=NA}
#library(jk.dusz.tarping)
zzz_mydata_cleaned <- jk.dusz.tarping::clean_my_data()

summary(zzz_mydata_cleaned)
```





## - Data exploration following Zuur *et al.* (2010)

In order to build valid and meaningful models to explain or predict the success/failure of tarping operations for the control of Japanese knotweed *s.l.*, we need first to learn more about our data. Therefore, we thoroughly examined our variables (e.g. outliers, distribution, interactions, colinearity, variance homogeneity, observation independence) mostly following the protocol presented by Zuur *et al.* (2010) for each of our **6 response variables**: *eff_eradication*, *efficiency*, *latest_reg_edges*, *reg_overlaps*, *latest_condition*, and *pb_fixation*.

As our global dataset contains many variables (*n* = 81), it would be interesting to reduce their number in order to spare time for further exploration and analyses. As such, the first steps of our data exploration for each response variable will aim at reducing the dimensionality of the data. First, we will try and see if the *environmental variables* can be coerced into a synthetic variable (e.g. through a PCA). Then, we will investigate correlations among variables as well as potential colinearity. This should have the twofold advantage of improving our understanding of the data and enabling us to remove colinear and too strongly correlated variables.



### - For *eff_eradication* and *efficiency* response variables

```{r erad import, include=FALSE}
erad <- jk.dusz.tarping::model_datasets(response.var = "efficiency")
erad <- 

erad %>% dplyr::select(-goals, -woven_geotex, -thickness) %>%
  dplyr::filter(eff_eradication != "NA") -> erad
```


#### - Investigation of correlation structures
##### - Multivariate synthesis of environmental variables

We performed a **normed-PCA** on some of our environmental variables for which we do not have strong hypotheses (i.e. *difficulty_access*, *shade*, *forest*, *ruggedness*, *granulometry*) but we kept *slope*, *obstacles*, and *flood* because we suspected they might have a major influence on the success/failure of tarping. In order to get optimum results, we imputed missing values using the *Regularised Iterative PCA algorithm* (Josse & Husson, 2013) of the `missMDA` package (Josse & Husson, 2016). 

```{r erad env PCAs, warning=FALSE, fig.align='center', fig.cap="**Figure 1.** Correlation plot of the normed-PCA on environmental variables for the *eradication efficiency* dataset"}
xxx <- erad[,6:12] %>% dplyr::select(-freq_monitoring, -slope) 

# As there are missing values in some variables, we will impute them using the missPCA() function.
# But first, we need to estimate how many components are required to correctly impute the missing values:
nb <- missMDA::estim_ncpPCA(xxx, ncp.max=5) # Can be time consuming. Here, the best result is 0 but we will take 2 as it is (almost) the second best option.
# Then we impute the missing values and extract the imputed dataset:
res.imput <- missMDA::imputePCA(X = xxx, method = "Regularized", scale = TRUE, ncp = 2)
xxx <- res.imput$completeObs # Gives the completed dataset!

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a large amount of the variance of my 5 variables (38.2%), I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
erad %>% dplyr::select(-difficulty_access, -shade, -forest, -ruggedness) %>%
  dplyr::rename(coarse_env = "granulometry") -> erad
erad$coarse_env <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.

# To plot side by side:
#gridExtra::grid.arrange(FIG_1, FIG_2, ncol = 2)
rm(res.pca, res.imput, xxx, ttt, nb)
```

As the first axis (PC) of our PCA satisfactorily synthesizes a large amount (38.2%) of the variance of the 5 environmental variables, we will use operations' coordinates on this axis to build a synthetic variable to summarize their environment. This new variable will be named **coarse_env** and will thus replace the other 5 variables in the dataset. Operations with *positive values* for this variable are located in sites that tend to be forested (and thus shaded), difficult to access, with uneven ground and a coarse soil texture (and vice-versa).


```{r, eval=FALSE, include=FALSE}
# PEUT ETRE INTERESSANT POUR D'AUTRES MODELES???????? mouais...
# pb <- zzz_mydata_cleaned[, c(59,60,64,66,68)] #PB+REG (2dim = 53%)
# pb <- na.omit(object = pb) # To have no obs. with NAs!
# summary(pb)
# 
# 
# #MCA
# acm <- FactoMineR::MCA(X = pb, ncp = 5,graph = FALSE)
# acm$eig
# plot(acm, label = "var")
# plot(acm, choix = "var")
```

  
##### - Correlation among variables

To investigate potential correlations and colinearity among our variables, we will start by computing a **correlation matrix** of the data. As most of our factors are either ordered factors (i.e. ordinal variables) or binary factors (i.e. boolean), we can use them in a correlation matrix. To do that however, computations will be based on *Spearman*'s rank correlation coefficient.

```{r erad corrplot, fig.align='center', fig.cap="**Figure 2.** Correlation matrix displaying *Spearman*'s $\\rho$ for the numeric variables and ordered factors of the *eradication efficiency* dataset. Only significant correlations are displayed (with $\\alpha$ = 0.05)", warning=FALSE}
# To convert all ordered factors into numeric variables usable in a correlation matrix:
erad %>% dplyr::select(-xp_id, -season, -preparation, -add_control_type) -> num.erad
num.erad <-  dplyr::mutate_if(.tbl = num.erad, .predicate = is.factor, .funs = as.character)
num.erad <-  dplyr::mutate_if(.tbl = num.erad, .predicate = is.character, .funs = as.numeric) # I use this double conversion-trick because, for an unknown reason, I failed to do it any other way!
#num.erad <- erad[, sapply(erad, is.numeric)] # Select only numeric columns.


# To compute the correlation matrix:
res.cor.erad <- round(stats::cor(num.erad, use = "complete.obs", method = "spearman"), 2)
# To compute a matrix of correlation p-values:
res.pcor.erad <- ggcorrplot::cor_pmat(x = num.erad, method = "spearman")

ggcorrplot::ggcorrplot(res.cor.erad, type = "upper",
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"), p.mat = res.pcor.erad, insig = "blank")

rm(num.erad, res.cor.erad, res.pcor.erad)
```

Many interesting patterns can be observed in this correlation matrix. Among them, we can see that:

* *eff_eradication* seems to be mildly positively correlated with *tarping_duration*, *sedicover_height*, *distance*, and *fully_tarped* and negatively with *obstacles*. 
* *add_control*, *rapairs* and *plantation* display a relatively similar pattern of positive correlation with *coarse_env* and negative correlation with *weedsp_geotex* and *maxveg*. However, this pattern certainly bears the mark of our own tarping experiments in Chalon that were installed in June and where we did not repaired set ups nor performed additional controls nor planted anything (most correlations with *weedsp_geotex* should also be influenced by our experiments).
* *pb_fixation* seems to be strongly positively correlated with *pla_geotex*.
* *stand_surface* seems to be strongly positively correlated with *flood*.

It is worth reminding that these correlations are just correlations (not **causation**) and only bivariate. Accounting for multivariate relationships and interactions will certainly give very different patterns.


   
##### - Multicolinearity



    
#### - Distributions and dispersion parameters
##### - Looking for outliers

We will look for potential outliers in the data using **boxplots** and **Cleveland dotplots** on our quantitative variables. 

```{r erad boxplot outliers, fig.align='center', fig.cap="**Figure 3.** Boxplots for all quantitative variables of the *eradication efficiency* dataset", warning=FALSE}
### IMPORTANT NOTE: as ggplot2 requires data in the long format to be able to do multi-panels plots (known as facets), we need to transform our data using the pivot_longer() function of {tidyr}. 
# To keep only numeric columns: num.erad <- erad[, sapply(erad, is.numeric)]; or we can do:

# With ggplot2, it is time-consuming!
#erad %>%
  #purrr::keep(is.numeric) %>% # Keep only numeric columns
  #tidyr::pivot_longer(cols = tidyselect::everything()) %>% # Pivot every columns
  #ggplot2::ggplot(ggplot2::aes(x = name, y = value)) +
  #ggplot2::geom_boxplot(fill = "lightsalmon") +
  #ggplot2::theme_minimal() -> ppp
#ppp + ggplot2::facet_wrap( ~ name, scales = "free")

#RColorBrewer::display.brewer.all() To display all the colors in {RColorBrewer}

jk.dusz.tarping::uni.boxplots(dataset = erad)
```

```{r erad dotplot, fig.align='center', fig.cap="**Figure 4.** Cleveland dotplots for all quantitative variables of the *eradication efficiency* dataset"}
jk.dusz.tarping::uni.dotplots(dataset = erad)
```

By the look of these graphs, it is clear that we have many *extreme values* (possible outliers) and non-Normal variables. Yet, extreme values are not necessarily **true outliers**:

* The variables *slope* and *coarse_env*  look fine.
* The variables *obstacles*, *flood*, *distance*, *strips_overlap*, *trench_depth*, and *tarping_duration* may require mild transformations to be normalized but do not seem to contain too extreme values.
* The variables *elevation*, *freq_monitoring*, *grammage*, *sedicover_height*, and *pierced_install* may require important transformations to be normalized. It would be interesting to see if their extreme values are too far from what could be expected from a Normal or a Poisson distribution (see below).
* The variables *latitude*, *longitude*, and *stand_surface* contain problematic patterns/outliers. As latitude and longitude are supposed to be used as ***random effects*** in our models, I do not know (yet) what would be the consequences of extreme values or bimodal distributions (but I guess it is not good).


```{r erad simu.distrib, include=FALSE, eval=FALSE}
simu.var <- dplyr::select(.data = erad, elevation, freq_monitoring, grammage, stand_surface, distance, strips_overlap, sedicover_height, trench_depth, pierced_tarpinstall, tarping_duration)

jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "log-normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "poisson")
rm(simu.var)

```

After analysing random samples drawn from log-normal distributions (based on the parameters of our potentially problematic variables), we have good reason to believe that only *latitude*, *longitude*, and *stand_surface* should be problematic. The other variables do not have distribution that a log-transformation could not fix.


   
##### - Homogeneity of variances

```{r}
dataset <- erad[, sapply(erad, is.numeric)]
dataset <- scale(x = dataset, center = T, scale = T)
#jk.dusz.tarping::uni.dotplots(dataset = dataset)

num.data <- dataset[, sapply(dataset, is.numeric)]
  nam <- names(num.data)
  ncol.data <- ncol(num.data)
  ncol.adjust <- ceiling(x = ncol.data/4) # Round to the next integer (e.g. ceiling(x = 7.12) returns 8)!
  num.data <- as.matrix(num.data)

  graphics::par(mfrow= c (ncol.adjust,4), mar=MAR, cex.lab = CEX.LAB, font.lab=FONT.LAB, bty = BTY, fg = FG,
      col.axis = COL.AXIS, col.lab = COL.LAB, cex = CEX.PAR, tcl = TCL,
      mgp = MGP, oma = OMA, lab = LAB)
  for (i in c(1:ncol(num.data))) {
    graphics::plot(x = num.data[,i], y = 1:length(num.data[,i]), type = "p", xlab = nam[i], ylab = "",
         col = COL.PCH, pch = PCH, panel.first = {
           grid(col=COL.GRID,nx = NX,ny = NY, lty = LTY)
         }, ...) }

```



Regarding scaling:
I will standardize my predictors, so there should be no problems (it is especially important when I use interactions terms: cf. "standardization" post on CV in my MacBook favorites)!
BTW, if I create an averaged variable (e.g. efficacy), I should perhaps standardise variables first to avoid giving to much weight to the variable with the largest values!
