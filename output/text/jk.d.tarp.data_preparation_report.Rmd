---
title: "Data Preparation Report - Knotweed's Tarping Survey (Dusz et al., 2020)"
author: "François-Marie Martin"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
   html_document:
     number_sections: yes
     toc: yes
     toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# - Introduction
## - Regarding reproductibility

In order to facilitate any attempt at reproducing this study, here is a depiction of the system information used to prepare and analyse the data from the survey on tarping operations for the control Japanese knotweed *s.l.* (*Reynoutria spp.*).  

```{r session information}
rm(list=ls())
sessionInfo()
```

The data and code used to analyse it can furthermore be found here: https://github.com/mrelnoob/jk.dusz.tarping
\
\

## - What this document is about?  

```{r data import and summary, include=FALSE}
library(jk.dusz.tarping)
zzz_mydata_cleaned <- jk.dusz.tarping::clean_my_data()

.pardefault <- par() # To save the default graphical parameters (in case I want to restore them).
```
\
In our efforts to build valid and meaningful models to explain or predict the success/failure of tarping operations for the control of Japanese knotweed *s.l.*, we had to learn more about our data. This *data preparation report* presents the results of the **exploratory data analyses** performed on the data we collected for this study.  
Here, we have explored various aspects of our data (e.g. outliers, variables distribution, multicollinearity among predictors, homogeneity of variances, independence, potential interactions, etc.) mostly following the protocol presented by Zuur *et al.* (2010). As we used slightly different datasets to model each of our **5 response variables** (i.e. *eff_eradication*, *efficiency*, *lreg_edges*, *lreg_tarpedarea*, *reg_overlaps*), we ran five separated exploratory analyses.

As our global dataset contains many potential predictors (*p* = 85), much effort was directed toward reducing their number and keeping only the most reliable and important variables relative to our hypotheses. In other words, we tried to reduce the dimensionality of the data. 
\
\

******

# - Exploration of the sub-dataset related to tarping *efficiency*
\
The first thing to do here was to prepare the sub-dataset that will be used to model the responses of the *efficiency variables* (i.e. **efficiency**, **eff_eradication** and **high_eff**). To do so, the following steps have been performed:

* We imported the subset of predictors and covariates that we wanted to use to model tarping efficiency (based on our knowledge and hypotheses).
* We left out observations that were too recent (tarping operations installed less than 2 growing seasons ago) and those for which we could not obtain an *efficiency score* (i.e. NA).
* We deleted operations that got prematurely interrupted (e.g. because of vandalism) since there failure results from external causes.
* Finally, we imputed missing values using **Random Forest** and the `missForest` package (Stekhoven & Buehlmann, 2012). 

The table below presents the *out-of-bag (OOB) imputation error* estimates for each type of variable (computed as the Normalized Root Mean Squared Error (NRMSE) for numeric variables, and the Proportion of Falsely Classified entries (PFC) for categorical variables). OOB values near 1 indicate that imputations are not reliable, while values close to 0 indicate very low imputation errors. For the meaning of the variables, please refer to the attached **documentation**.

```{r erad import & imput, include=FALSE}
erad <- jk.dusz.tarping::model_datasets(response.var = "efficiency")

erad %>% dplyr::select(-liner_geomem, -agri_geomem,-woven_geotex, -mulching_geotex, -pla_geotex, -other_unknown, -grammage, -age, -thickness, -pierced_tarpinstall, -add_control_type) %>%
  dplyr::filter(eff_eradication != "NA") %>%
  dplyr::filter(tarping_duration >= 2) %>%
  dplyr::filter(xp_id != "54") -> erad # Operation n°54 was destroyed because of vandalism.

### Imputation of missing values using `missForest` (we could also have worked with `MICE`):
erad %>% dplyr::select(-manager_id, -xp_id) %>% as.data.frame() -> erad_mis # missForest only accepts data.frames or matrices (not tibbles). Variables must also be numeric or factors only (no character, date, difftime... classes)!

imput <- missForest::missForest(xmis = erad_mis, verbose = TRUE)

# To create a small summary table for the OOB errors (for each variable, with the argument `variablewise = TRUE` in missForest):
# imput_error <- data.frame(cbind(
#   sapply(erad_mis, function(y) sum(length(which(is.na(y))))), # Number of imputed values (NAs)
#   imput$OOBerror), # Out-of-bag error (OOB) for each variable
#   row.names = colnames(erad_mis)) # To get the name of the variables
# dplyr::rename(imput_error, nb_imputed_values = 'X1', oob_error = 'X2') -> imput_error
```

```{r erad imput error table, include=TRUE, message=FALSE, comment=NA}
knitr::kable(imput$OOBerror)
```

Here are a few descriptive statistics of our sub-dataset.

```{r erad summary, include=TRUE, message=FALSE, comment=NA}
erad[,3:ncol(erad)] <- imput$ximp # Replacing th original dataset with the imputed one!
summary(erad)

rm(erad_mis, imput)
```
\
\

## - Investigation of correlation structures
### - Multivariate synthesis of some contextual variables
\
We performed a **normed-PCA** on some of our environmental variables for which we did not have strong direct hypotheses (i.e. *difficulty_access*, *shade*, *forest*, *ruggedness*, *granulometry*) but we kept *slope*, *obstacles*, and *flood* because we suspected they might have a major influence on the success/failure of tarping. 

```{r erad env PCAs, warning=FALSE, fig.align='center', fig.cap="**Figure 2.1.** Correlation plot of the normed-PCA on environmental variables for the *tarping efficiency* dataset"}
xxx <- dplyr::select(.data = erad, c(difficulty_access, shade, forest, ruggedness, granulometry))

# In order to get optimum results, we imputed missing values using the *Regularised Iterative PCA algorithm* (Josse & Husson, 2013) of the `missMDA` package (Josse & Husson, 2016). 
# But first, we need to estimate how many components are required to correctly impute the missing values:
#nb <- missMDA::estim_ncpPCA(xxx, ncp.max=5) # Can be time consuming. Here, the best result is 0 but we will take 2 as it is (almost) the second best option.
# Then we impute the missing values and extract the imputed dataset:
#res.imput <- missMDA::imputePCA(X = xxx, method = "Regularized", scale = TRUE, ncp = 2)
#xxx <- res.imput$completeObs # Gives the completed dataset!

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a fairly large amount of the variance of my 5 variables, I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
erad %>% dplyr::select(-difficulty_access, -shade, -forest, -ruggedness) %>%
  dplyr::rename(coarse_env = "granulometry") -> erad
erad$coarse_env <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.

# To plot side by side:
#gridExtra::grid.arrange(FIG_1, FIG_2, ncol = 2)
```

As the first axis (PC) of the PCA satisfactorily synthesized a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of these 5 environmental variables, we used operations' coordinates on this axis to build a synthetic variable to summarize their environment. This new variable was named **coarse_env** and replaced the other 5 variables in the dataset. Operations with *positive values* for this variable were located in sites that tended to be forested (and thus shaded), difficult to access, with uneven ground and a coarse soil texture (and vice-versa).

We also performed another **normed-PCA** on some of our follow-up variables that we thought could be synthesized (i.e. *add_control*, *repairs*, and *freq_monitoring*).


```{r erad followups, warning=FALSE, fig.align='center', fig.cap="**Figure 2.1 bis.** Correlation plot of the normed-PCA on follow-up variables for the *tarping efficiency* dataset"}
erad %>% dplyr::select(freq_monitoring, repairs, add_control) %>%
  dplyr::mutate(repairs = as.numfactor(x = repairs),
                add_control = as.numfactor(x = add_control))-> xxx

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a fairly large amount of the variance of my 5 variables, I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
erad %>% dplyr::select(-freq_monitoring, -repairs) %>%
  dplyr::rename(followups = "add_control") -> erad
erad$followups <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.
```

As the first axis (PC) of the PCA satisfactorily synthesized a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of the 3 follow-up variables, we used operations' coordinates on this axis to build a synthetic variable to summarize their follow-up conditions. This new variable was named **followups** and thus replaced the other 3 variables in the dataset. High *positive values* for this variable represent operations that were frequently monitored and where additional control and repairs were performed (and vice-versa).
\
\

### - Correlation among variables
\
To investigate potential correlations and collinearity among our variables, we started by computing a **correlation matrix** of the data. As most of our factors were either ordered factors (i.e. ordinal variables) or binary factors (i.e. boolean), we could use them in a correlation matrix. To do that however, computations were based on *Spearman*'s rank correlation coefficients.  
To avoid giving too much weight to our own tarping experiments (those performed in Chalon s/ Saone with the French national railway company), we removed them from the data to compute correlations: their influence was accounted for in further analyses by the use of **mixed-effect models**.

```{r erad corrplot, fig.align='center', fig.cap="**Figure 2.2.** Correlation matrix displaying *Spearman*'s $\\rho$ for the numeric variables and ordered factors of the *tarping efficiency* dataset. Only significant correlations are displayed (with $\\alpha$ = 0.05)", warning=FALSE}
rm(res.pca, res.imput, xxx, ttt, nb)

# To convert all ordered factors into numeric variables usable in a correlation matrix:
erad %>% dplyr::filter(!manager_id == 1) %>% # We remove our own experiments as they share many features that will strongly influence correlations (it should not be a problem for modelling as we will use mixed-effect models).
  dplyr::select(-manager_id, -xp_id, -goals) %>%
  dplyr::mutate_if(.predicate = is.factor, .funs = as.numfactor) -> num.erad # as.numfactor() is my own custom function (see '01_data_cleaning.R').



# To compute the correlation matrix:
res.cor.erad <- round(stats::cor(num.erad, use = "complete.obs", method = "spearman"), 2)
# To compute a matrix of correlation p-values:
res.pcor.erad <- ggcorrplot::cor_pmat(x = num.erad, method = "spearman")

ggcorrplot::ggcorrplot(res.cor.erad, type = "upper",
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"), p.mat = res.pcor.erad, insig = "blank")

rm(num.erad, res.cor.erad, res.pcor.erad)
```

Many interesting patterns can be observed in this correlation matrix. Among them, we can see that:

* One or several of the *efficiency* variables seem to be mildly positively correlated with *distance*, and *fully_tarped* and negatively with *obstacles*, *followups*, and *pb_durability*. 
* *geomem* is extremely negatively correlated with *geotex*, which is logical as one is the opposite of the other. 
* *trench_depth* seems to be strongly positively correlated with *tarpfix_multimethod*, which is logical as trenches are strong fixation method. 
* *pb_fixation* seems to be strongly positively correlated with *stand_surface* and, to a lesser extent, with *tarpfix_pierced*.
* *obstacles* is positively correlated with *slope* and *coarse_env* (which are also positively correlated), as well as with *pb_durability*, and negatively with *distance*: all these correlations seem quite straightforward. 
* Interestingly, *flood* is positively correlated with *geotex*, *tarpfix_pierced*, and *pb_fixation*.

It is worth reminding that accounting for multivariate relationships, interactions and the effect of our tarping operations (dropped for the current correlation matrix) will certainly affect the observed patterns.
\
\
   
### - Multicollinearity
\
As multicollinearity is one of the biggest issues for inference modelling, we wanted to assess which explanatory variables and covariates were collinear and would possibly become problematic during modelling stages. To do so, our intention was to compute *Variance Inflation Factors* (VIF) for each variable. However, as several variables were categorical and as we will not be working with models fitted with ordinary least squares, we computed *Generalized VIF* values instead (Fox & Monette, 1992).

```{r erad init GVIF}
modvif <- stats::glm((efficiency/10)~latitude+longitude+elevation+slope+coarse_env+obstacles+flood+geomem+geotex+weedsp_geotex+maxveg+uprootexcav+stand_surface+fully_tarped+distance+tarping_duration+stripsoverlap_ok+tarpfix_multimethod+tarpfix_pierced+sedicover_height+trench_depth+plantation+followups+degradation+pb_fixation+pb_durability, data = erad, family = quasibinomial)
#summary(modvif)

knitr::kable(car::vif(modvif), digits = 3, col.names = "GVIF", caption = "Table 1. Test")
```

As one could expect, there is evidence for **multicollinearity** among our 26 potential explanatory variables. However, strong collinearity among variables is only a problem if these variables are included together in the same model, hence we should reevaluate collinearity more precisely when actual working models will be built. Here, we will simply identify which variables will be generally too problematic across likely models and, possibly, drop them (we will try to lower GVIF values as much as possible, considering that, even GVIF of 2 could still be problematic if ecological signals are weak; cf. Zuur *et al.*, 2010):

* The *geomem* and *geotex* (and *weedsp_geotex*) variables are, without a surprise, problematic as they contain the same information. Therefore, we decide to only keep *geomem*.
* The *plantation* variable also seems highly problematic. Yet, as we have a strong interest in that variable, we choose to drop it **only temporarily**, only to detect other problematic variables. A particular attention to multicollinearity should be maintained when this variable will be included within models.
* Since our dataset contains sites on both sides of the Atlantic and since we do not have strong hypotheses regarding the effects of *longitude* (because our sites are located well within the longitudinal range of knotweeds), we decide to drop it.
* Since *degradation* contains most of the information contained in *pb_fixation* and *pb_durability*, we decide to drop it to reduce the number of variables. 

After iteratively dropping these collinear variables and re-assessing GVIF values, we obtained a subset of potential explanatory variables with GVIF values globally < 2.5. Naturally, it will be important not to forget about the potential effect of the dropped variables in the discussion of our results.

```{r erad final GVIF}
modvif <- stats::glm((efficiency/10)~latitude+elevation+slope+coarse_env+obstacles+flood+geomem+maxveg+uprootexcav+stand_surface+fully_tarped+distance+tarping_duration+stripsoverlap_ok+tarpfix_multimethod+tarpfix_pierced+sedicover_height+trench_depth+followups+pb_fixation+pb_durability, data = erad, family = quasibinomial)
#summary(modvif)

knitr::kable(car::vif(modvif), digits = 3, col.names = "GVIF")

erad %>% dplyr::select(-geotex, -weedsp_geotex, -longitude, -degradation) -> erad

rm(modvif)
```
\
\
    
## - Distributions and dispersion parameters
### - Looking for outliers
\
The next step in our data exploration was to look for potential outliers in the data using **boxplots** and **Cleveland dotplots** on our quantitative variables. 

```{r erad boxplot outliers, fig.align='center', fig.cap="**Figure 2.3.** Boxplots for all quantitative variables of the *eradication efficiency* dataset", warning=FALSE}
### IMPORTANT NOTE: as ggplot2 requires data in the long format to be able to do multi-panels plots (known as facets), we need to transform our data using the pivot_longer() function of {tidyr}. 
# To keep only numeric columns: num.erad <- erad[, sapply(erad, is.numeric)]; or we can do:

# With ggplot2, it is time-consuming!
#erad %>%
  #purrr::keep(is.numeric) %>% # Keep only numeric columns
  #tidyr::pivot_longer(cols = tidyselect::everything()) %>% # Pivot every columns
  #ggplot2::ggplot(ggplot2::aes(x = name, y = value)) +
  #ggplot2::geom_boxplot(fill = "lightsalmon") +
  #ggplot2::theme_minimal() -> ppp
#ppp + ggplot2::facet_wrap( ~ name, scales = "free")

#RColorBrewer::display.brewer.all() To display all the colors in {RColorBrewer}

jk.dusz.tarping::uni.boxplots(dataset = erad)
```

```{r erad dotplot, fig.align='center', fig.cap="**Figure 2.4.** Cleveland dotplots for all quantitative variables of the *eradication efficiency* dataset"}
jk.dusz.tarping::uni.dotplots(dataset = erad)
```

By the look of these graphs, it is clear that we have many *extreme values* (possible outliers). Yet, extreme values are not necessarily **true outliers**:

* The *latitude*, *slope* and *coarse_env* variables look fine.
* The *efficiency*, *obstacles*, *flood*, *distance*, *tarping_duration*, *trench_depth*, and *followups* variables may require mild transformations to be normalized but do not seem to contain too extreme values.
* The *elevation* and *sedicover_height* variables may require important transformations to be normalized. It would be interesting to see if their extreme values are too far from what could be expected from a Normal or a Poisson distribution (see below).
* The *stand_surface* variable contains problematic patterns/outliers.


```{r erad simu.distrib, include=FALSE, eval=FALSE}
simu.var <- dplyr::select(.data = erad, elevation, freq_monitoring, stand_surface, distance, sedicover_height, trench_depth, tarping_duration)

jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "log-normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "poisson")
rm(simu.var)

```

After analysing random samples drawn from log-normal distributions (based on the parameters of our potentially problematic variables), we have good reason to believe that only *stand_surface* should be problematic. Consequently, we chose to delete the spotted outlier within *stand_surface* (i.e. the *stand_surface* of ca. 5000 m^2^).    
The other variables do not seem have values that are too extreme to be possibly drawn from Normal, log-Normal or Poisson distributed variables. 


```{r erad update, include=FALSE}
erad <- erad[-(which.max(erad$stand_surface)),]
```
\
\
   
### - Normality
\
Although we already gained pretty good insights into the probability distribution followed by our variables, it is usually interesting and often useful to have a precise view of the variables' distributions.
Consequently, we described the shapes of the distribution of our quantitative variables using histograms as well as skewness and kurtosis values. Skewness is a measure of the symmetry of a variable's distribution (a perfectly normal distribution has a skewness of 0), while kurtosis gives information on the combined amount of probability contained in the two tails of a distribution (often reported as *excess kurtosis*, that is kurtosis - 3, since a perfectly normally distributed variable has a kurtosis of 3). There is much debate regarding what constitutes "acceptable" values for these statistics or whether this question actually makes any sense at all. Yet, various authors suggest that values of ±2 should not be too problematic for procedures requiring approximately normally distributed variables (Field, 2009; Gravetter & Wallnau, 2014). Nonetheless, many statistical methods do not possess a direct normality assumption (linear regression models, for instance and contrarily to popular beliefs, do not assume the normality of either the response variable nor of any predictor/covariate but do assume that the replicated observations of the response variable for any fixed predictor value should be normally distributed (Montgomery & Peck, 1992); an assumption that is often difficult to verify, hence the usual focus on models residuals).
For all these reasons, here, we only used these statistics as descriptive tools to foresee potential future complications related to the distribution of our variables, but not as bases for actual decision making. 

```{r erad normality, fig.align='center', fig.cap="**Figure 2.5.** Histograms for all quantitative variables of the *eradication efficiency* dataset"}
uni.histograms(dataset = erad)

num.data <- erad[, sapply(erad, is.numeric)]
tab <- data.frame(moments::skewness(x = num.data), moments::kurtosis(x = num.data)-3)
knitr::kable(x = tab, digits = 3, col.names = c("Skewness", "Excess kurtosis"))
rm(num.data, tab)
```

From the above histograms and table, we can see that:
* Very few of our sampled variables are close to approximate a normal distribution: perhaps *latitude*, *coarse_env*, and *followups*. Surprisingly, *efficiency* possesses "acceptable" skewness and kurtosis values yet is clearly far from a normal variable (left-dissymmetric and bounded), perhaps attesting the limited usefulness of these statistics.
* Most variables are skewed to the right but in relatively reasonable proportions. 
* Some variables (*elevation*, *stand_surface*, *distance*, and *sedicover_height*) have skewness and/or kurtosis values possibly high enough to have undue influence on regression coefficients. These variables will perhaps require some transformations.
\
\

## - Multivariate relationships
### - Relationships among variables
\
The next step in our data exploration was to plot the response variables against each predictors and covariates in order to assess the **linearity** and **homoscedasticity** of these relationships and identify potential **multivariate outliers**. It would have been interesting to plot all variables against each other, but the number of potential predictors and covariates was too high for that.

```{r erad bivariate plot, fig.align='center', fig.cap="**Figure 2.6.** Multivariate representation of the *efficiency score* against each potential explanatory variable with a focus on whether knotweed stands were fully tarped or not", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
### To plot all variables against each other, we can use pairs() or ggpairs():
# ggplot2::theme_set(ggplot2::theme_bw())
# erad %>% dplyr::select(-xp_id, -goals) %>% GGally::ggpairs()

### To plot a response variable against all predictors/covariates, we will have to do use a for loop or the facet_wrap function of ggplot (or equivalent).
# For the "efficiency" variable (bounded continuous variable):
erad %>% dplyr::select(-manager_id, -xp_id, -goals, -eff_eradication, -high_eff) %>%
  dplyr::mutate_if(is.factor, as.numfactor) %>% 
  tidyr::pivot_longer(-c(efficiency, fully_tarped), # Means that all columns of `mydata` need to be reshaped except "efficiency" & "fully_tarped" (works as well with the form `!efficiency`, and we can place it somewhere else: actually, it is the `cols =` argument)! We have to work in the long-format because that's how the tidyverser and thus ggplot2 love to work...
                               names_to = "Predictor", # Gives the name of the new grouping variable while "values_to" gives the name of the variable that will be created from the data stored in the cell value.
                               values_to = "Value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = Value, y = efficiency)) +
  ggplot2::facet_wrap(~Predictor, scales = "free_x", strip.position = "bottom") +
  ggplot2::geom_point(size = 1.5, alpha = 0.8, ggplot2::aes(col = as.factor(fully_tarped))) +
  ggplot2::scale_y_continuous(limits=c(0,10)) +
  ggplot2::geom_smooth(method = "lm", na.rm = TRUE, size = 1, 
                       ggplot2::aes(col = as.factor(fully_tarped))) + # If you want to affect something related to the input dataframe (such as here, where we want a smoother for each value of "fully_tarped"), you have to put the command in an aes() function!
# If a "loess" smoother is used, sometimes the smoother line won't be plotted, I don't know why.
  ggplot2::scale_colour_manual(values = c("plum1", "palevioletred4")) +
  ggplot2::labs(x = "Predictor value", y = "Efficiency score", col = "Fully tarped") +
  ggthemes::theme_hc(style = "darkunica") +
  ggplot2::theme(panel.border = ggplot2::element_blank(),
          legend.key = ggplot2::element_blank(),
          strip.background = ggplot2::element_blank(),
          strip.text = ggplot2::element_text(colour = "gray60", size = 10),
          strip.placement = "outside",
          axis.line.x = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          axis.line.y = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          legend.position = "right")
```

Various interesting observations can be made from these scatterplots:

* First, heteroscedasticity will perhaps not be surprising in our models as the variances of many variables do not seem to be homogeneous. On the other hand, we do not see clear multivariate outliers.
* Second, the *efficiency* score seems to increase with increasing *distance*. This may be evidence in support to one of our working hypotheses stating that successful tarping operations cover areas well beyond the visible limits of knotweed stands. Conversely, the *efficiency* score seems to decrease with increasing presence of *obstacles* and with *pb_durability*.
* Third, surprisingly, the *efficiency* score also seems to increase with increasing *elevation*. If this is not due to measurement error, then it could be a sign of stronger abiotic constraints on knotweeds. *Follow-ups* similarly seems to improve *efficiency*.
* Fourth, there may be signs of interesting interactions between the *fully tarped* variable and *coarse_env*, *pb_durability*, *slope*, *stand_surface*, *tarpfix_pierced*, *tarping_duration*, *trench_depth* and *uprootexcav*, although caution is required as variances are quite high. Furthermore, if interactions with *stand_surface* or *pb_durability* are relatively straightforward, the other ones are trickier do grasp and could simply be spurious.

To further investigate multivariate relationship, we created another set of scatterplots (but we did not intend on investigating all possible combinations between variables).

```{r erad bivariate plot2, fig.align='center', fig.cap="**Figure 2.6bis.** Multivariate representation of the *efficiency score* against each potential explanatory variable with a focus on whether knotweed stands were fully tarped or not", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
erad %>% dplyr::select(-manager_id, -xp_id, -goals, -eff_eradication, -high_eff) %>%
  dplyr::mutate_if(is.factor, as.numfactor) %>% 
  tidyr::pivot_longer(-c(efficiency, plantation), 
                               names_to = "Predictor", 
                               values_to = "Value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = Value, y = efficiency)) +
  ggplot2::facet_wrap(~Predictor, scales = "free_x", strip.position = "bottom") +
  ggplot2::geom_point(size = 1.5, alpha = 0.8, ggplot2::aes(col = as.factor(plantation))) +
  ggplot2::scale_y_continuous(limits=c(0,10.5)) +
  ggplot2::geom_smooth(method = "lm", na.rm = TRUE, size = 1, 
                       ggplot2::aes(col = as.factor(plantation))) + 
  ggplot2::scale_colour_manual(values = c("coral", "darkred")) +
  ggplot2::labs(x = "Predictor value", y = "Efficiency score", col = "Plantation") +
  ggthemes::theme_hc(style = "darkunica") +
  ggplot2::theme(panel.border = ggplot2::element_blank(),
          legend.key = ggplot2::element_blank(),
          strip.background = ggplot2::element_blank(),
          strip.text = ggplot2::element_text(colour = "gray60", size = 10),
          strip.placement = "outside",
          axis.line.x = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          axis.line.y = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          legend.position = "right")
```

We can see from this new panel of plots that the *efficiency* score seems to increase when knotweeds are *fully tarped*, as we hypothesized. 
\
\
   
### - Homogeneity of variances

To further investigate potential **heteroscedasticity** and need for transformations, we also plotted conditional boxplots of our only continuous response variable (i.e. *efficiency*).

```{r erad cond.boxplots, fig.align='center', fig.cap="**Figure 2.7.** Bivariate conditional boxplots of the *efficiency score* against each potential explanatory variable", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
erad %>% dplyr::select(-goals) -> erad1
jk.dusz.tarping::cond.boxplots(dataset = erad1, Y = 3, Xs = 6:ncol(erad1), outlier = TRUE)
```

According to Fox (2008), for simple linear regression models, problematic heterogeneity of variance occurs when the ratio between the largest and smallest variance is 4 or more. Consequently, there is a possibility for the following variables to become problematic for modelling: *elevation*, *flood*, *stand_surface*, *tarping_duration*, *sedicover_height*, and *trench_depth*. Nonetheless, actual problematic heteroscedasticity will only be observable by plotting residuals against models fitted values.
\
\
   
### - Interactions among variables

Finally, to further investigate potentially meaningful **interactions** among our variables, we used co-plots (i.e. conditioning plots) between some of our predictors, based on plausible hypotheses. However, to avoid overloading the present document, we only display some of the most informative plots. 

```{r erad coplot 1, fig.align='center', fig.cap="**Figure 2.8a.** Coplot of the *efficiency score* displaying a three-way interaction between *elevation*, *fully_tarped* and *stand_surface* (in log-scale). There is a 5% overlap between each group, explaining why there are more than *n* = 85 points. Colours are meaningless and are just here for aesthetical reasons", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}

par(.pardefault) # To restore defaults graphical parameters

### Three-way interaction between elevation, fully_tarped, and stand_surface
graphics::coplot(efficiency ~ elevation | fully_tarped * log(stand_surface), data = erad, 
                 panel=function(x,y,...) { points(x,y, col = wesanderson::wes_palette(n=4, name="GrandBudapest1"), pch = 19); abline(line(y~x)) }, # The function in the `panel` argument is what enables the drawing of the regression lines! In the col argument, I simply created a palette of colours based on the movies of Wes Anderson (just to make the plot prettier).
                 number = 3, overlap = 0.05) # `number` indicates the number of groups by which the quantitative grouping variable (here, stand_surface) should be divided into. The `overlap` argument controls the proportion of points that are shared between the groups (here, 5%).
# Note that, sometimes coplot will create several rows for the same variable, but this can be controlled using the `rows` argument (e.g. rows = 1).
```

This coplot is very interesting because it shows an inconsistent pattern within each category (single row or single column) suggesting that our sample prevents us to satisfactorily test a three-way interaction between those variables. Additionally, we can see that there are very few observations of not entirely tarped knotweed stands (*n* = 17) as well as very few observations above 500m a.s.l. (*n* = 9; which is not a very high elevation for knotweeds (Martin *et al.*, 2019)). As our dataset was gathered opportunistically it is not surprising, yet it means that our sampled dataset will only be able to highlight strong effects of elevation and of partially tarped control operations, which is not the case here. All this may explain the very odd pattern displayed by the lower-right panel: Figure 2.2, Figure 2.6 and the other panels (save one) seem to indicate that there is a positive correlation between elevation and tarping efficiency; however, the fact that this pattern is reversed for smaller knotweed stands (and thus weaker ones) that are entirely covered does not make any sense. It is therefore possible that the seemingly higher efficiency of tarping at higher elevations actually reflects chance.

As we already have too many candidate predictors, we may consider dropping the *elevation* variable for further analyses. 

```{r erad coplot 2, fig.align='center', fig.cap="**Figure 2.8b.** Coplot of the *efficiency score* displaying a two-way interaction between *fully_tarped* and *stand_surface* (in log-scale). There is a 5% overlap between each group, explaining why there are more than *n* = 85 points. Colours are meaningless and are just here for aesthetical reasons", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
### 
graphics::coplot(efficiency ~ log(stand_surface) | fully_tarped, data = erad, 
                 panel=function(x,y,...) { points(x,y, col = wesanderson::wes_palette(n=4, name="GrandBudapest2"), pch = 19); abline(line(y~x)) }, 
                 number = 4, overlap = 0.05, rows = 1)
```



```{r}



# Interactions à tester:

distance*surface # NOPE (here)
surface*fully_tarped # Eventuellement: assez intéressant (le fait bacher en entier supprime l'effet négatif de la taille des taches sur l'efficacité)

distance | surface*fullyT
surface | distance*followups
distance*obstacle
distance*slope
obstacles | distance*followups

geomem*followups
maxveg*surface
uproot*surface
uproot/maxvag * plantation

time*pb_durab
time*followups
time | followups * PBs (les 2)

trench*surface

surface*plantation
slope*plantation
plantation*pb_durab
#AVEC GOALS???? est-ce que ceux qui veulent exterminer y arrivent mieux?

### Pour plotter des régressions logistiques
fit = glm(vs ~ hp, data=mtcars, family=binomial)
newdat <- data.frame(hp=seq(min(mtcars$hp), max(mtcars$hp),len=100))
newdat$vs = predict(fit, newdata=newdat, type="response")
plot(jitter(vs,amount = 0.03)~hp, data=mtcars, col="red4")
lines(vs ~ hp, newdat, col="green4", lwd=2)

```





```{r erad export, include=FALSE}
### Finally, I export the dataset to be used for modelling in my "mydata" folder:
readr::write_csv(x = erad, file = here::here("mydata", "erad.csv"), append = FALSE)
# A problem with this method is that once exported, R will lose track of the variable types (class). Therefore, if we open (read) this .csv file with R, it will specify columns as best it can, that is as double for numeric columns and as character for columns containing letters. For instance, it will import the first column of `erad` (i.e. "manager_id") as a numeric (double) variable when it should be a factor.
# A not so ideal solution is to use the `col_types` argument of the readr::read_csv() function to manually specify variable types during import. Yet, we will not be able to specify that "plantation" is an ordered factor and we will have to do it manually afterward. 
```
\
\

******

### - For the response variables indicating knotweed regrowths at the edge of the tarped area (i.e. *lreg_edges*)
\
The first thing to do here is to prepare the sub-dataset that will be used to model the responses of the *lreg_edges* variable. To do so, the following steps have been performed:

* We selected the subset of predictors and covariates that will be used to model *lreg_edges*;
* We left out the observations for which we could not obtain information on regrowths (i.e. NA);
* Finally, we imputed missing values using **Random Forest** and the `missForest` package (Stekhoven & Buehlmann, 2012). 

The table below presents the *out-of-bag (OOB) imputation error* estimates for each type of variables (computed as the Normalized Root Mean Squared Error (NRMSE) for the numeric variables, and Proportion of Falsely Classified entries (PFC) for the categorical variables). OOB values near 1 indicate that imputations are not reliable, while values close to 0 indicate very low imputation error. For the meaning of the variables, please refer to the attached **documentation**.

```{r redges import & imput, include=FALSE}
redges <- jk.dusz.tarping::model_datasets(response.var = "edges")
redges <- dplyr::mutate(.data = redges, 
                        lreg_edges = as.factor(x = lreg_edges),
                        reg_elsewhere = as.factor(x = reg_elsewhere),
                        uprootexcav = as.factor(x = uprootexcav))

### Imputation of missing values using `missForest` (we could also have worked with `MICE`):
redges %>% dplyr::select(-manager_id, -xp_id) %>% as.data.frame() -> redges_mis # missForest only accepts data.frames or matrices (not tibbles). Variables must also be numeric or factors only (no character, date, difftime... classes)!

imput <- missForest::missForest(xmis = redges_mis, verbose = TRUE)
```

```{r redges imput error table, include=TRUE, message=FALSE, comment=NA}
knitr::kable(imput$OOBerror)
```

Now, let's take a quick look at our dataset.

```{r redges summary, include=TRUE, message=FALSE, comment=NA}
redges[,3:ncol(redges)] <- imput$ximp # Replacing the original dataset with the imputed one!
summary(redges)

rm(redges_mis, imput)
```
\
\

#### - Investigation of correlation structures
##### - Multivariate synthesis of environmental variables
\
We performed a **normed-PCA** on some of our environmental variables for which we do not have strong hypotheses (i.e. *difficulty_access*, *shade*, *forest*, *ruggedness*, *granulometry*) but we kept *slope*, *obstacles*, and *flood* because we suspected they might have a strong influence on the presence of knotweed regrowths. 

```{r redges env PCAs, warning=FALSE, fig.align='center', fig.cap="**Figure 7.** Correlation plot of the normed-PCA on environmental variables for the *regrowth at the edges of the tarped area* dataset"}
xxx <- dplyr::select(.data = redges, c(difficulty_access, shade, forest, ruggedness, granulometry))

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a fairly large amount of the variance of my 5 variables, I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
redges %>% dplyr::select(-difficulty_access, -shade, -forest, -ruggedness) %>%
  dplyr::rename(coarse_env = "granulometry") -> redges
redges$coarse_env <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.
```

As the first axis (PC) of our PCA satisfactorily synthesizes a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of the 5 environmental variables, we will use operations' coordinates on this axis to build a synthetic variable to summarize their environment. This new variable will be named **coarse_env** and will thus replace the other 5 variables in the dataset. Operations with *positive values* for this variable are located in sites that tend to be forested (and thus shaded), difficult to access, with uneven ground and a coarse soil texture (and vice-versa).

```{r redges followups, warning=FALSE, fig.align='center', fig.cap="**Figure 1 bis.** Correlation plot of the normed-PCA on follow-up variables for the *tarping efficiency* dataset"}
redges %>% dplyr::select(freq_monitoring, repairs, add_control) %>%
  dplyr::mutate(repairs = as.numfactor(x = repairs),
                add_control = as.numfactor(x = add_control))-> xxx

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a fairly large amount of the variance of my 5 variables, I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
redges %>% dplyr::select(-freq_monitoring, -repairs) %>%
  dplyr::rename(followups = "add_control") -> redges
redges$followups <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.
```

As the first axis (PC) of our PCA satisfactorily synthesizes a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of the 3 follow-up variables, we will use operations' coordinates on this axis to build a synthetic variable to summarize their follow-up. This new variable will be named **followups** and will thus replace the other 3 variables in the dataset. High *positive values* for this variable represent operations that were frequently monitored and where additional control and repairs were performed (and vice-versa).
\
\
  
##### - Correlation among variables
\
To investigate potential correlations and colinearity among our variables, we will start by computing a **correlation matrix** of the data. As most of our factors are either ordered factors (i.e. ordinal variables) or binary factors (i.e. boolean), we can use them in a correlation matrix. To do that however, computations will be based on *Spearman*'s rank correlation coefficient.  
To avoid giving too much weight to our own tarping experiments (those performed in Chalon s/ Saone with the French national railway company), we removed them from the data to compute correlations: their influence will be accounted for in further analyses by the use of **mixed-effect models**.

```{r redges corrplot, fig.align='center', fig.cap="**Figure 8.** Correlation matrix displaying *Spearman*'s $\\rho$ for the numeric variables and ordered factors of the *regrowth at the edges of the tarped area* dataset. Only significant correlations are displayed (with $\\alpha$ = 0.05)", warning=FALSE}
rm(res.pca, xxx, ttt)

# To convert all ordered factors into numeric variables usable in a correlation matrix:
redges %>% dplyr::filter(!manager_id == 1) %>% # We remove our own experiments as they share many features that will strongly influence correlations (it should not be a problem for modelling as we will use mixed-effect models).
  dplyr::select(-manager_id, -xp_id) %>%
  dplyr::mutate_if(.predicate = is.factor, .funs = as.numfactor) -> num.redges # as.numfactor() is my own custom function (see '01_data_cleaning.R').



# To compute the correlation matrix:
res.cor.redges <- round(stats::cor(num.redges, use = "complete.obs", method = "spearman"), 2)
# To compute a matrix of correlation p-values:
res.pcor.redges <- ggcorrplot::cor_pmat(x = num.redges, method = "spearman")

ggcorrplot::ggcorrplot(res.cor.redges, type = "upper",
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"), p.mat = res.pcor.redges, insig = "blank")

rm(num.redges, res.cor.redges, res.pcor.redges)
```

Many interesting patterns can be observed in this correlation matrix. Among them, we can see that:

* The *lreg_edges* variable seems to be negatively correlated with *distance*, as we hypothesized. 
* *geomem* is extremely negatively correlated with *geotex*, which is logical as one is the opposite of the other. 
* *tarpfix_multimethod* seems to be strongly positively correlated with *trench_depth*, *tarpfix_pierced* and *plantation*, which is logical as trenches, staples and plantations are considered strong fabric fixation methods. 
* Without surprise, *degradation* seems to be strongly positively correlated with *pb_fixation* and *pb_durability*.
* *stand_surface* seems positively correlated with *flood*, *geotex*,*distance*, and *pb_fixation*.
* *obstacles* is positively correlated with *slope* and *coarse_env* (which are also positively correlated), as well as with *pb_durability* and *reg_elsewhere*.
* *followups* is positively correlated with *degradation* and *reg_elsewhere*.

It is worth reminding that accounting for multivariate relationships, interactions and the effect of our tarping operations (dropped for the current correlation matrix) will certainly give very different patterns.
\
\
   
##### - Multicollinearity
\
As multicollinearity is one of the biggest issues for inference modelling, we need to assess which explanatory variables and covariates are collinear and will possibly become problematic for modelling. To do so, our intention was to compute *Variance Inflation Factors* (VIF) for each variable. However, as several variables are categorical and as we will not be working with models fitted with ordinary least squares, we will rather use *Generalized VIF* (Fox & Monette, 1992).

```{r redges init GVIF}
modvif <- stats::glm(lreg_edges~latitude+longitude+elevation+slope+flood+coarse_env+obstacles+stand_surface+geomem+geotex+maxveg+uprootexcav+distance+stripsoverlap_ok+tarpfix_multimethod+tarpfix_pierced+sedicover_height+trench_depth+plantation+followups+degradation+pb_fixation+pb_durability+tarping_duration+reg_elsewhere, data = redges, family = binomial)
#summary(modvif)

knitr::kable(car::vif(modvif))
```

As one could expect, there is evidence for **multicollinearity** among our 27 potential explanatory variables. However, strong collinearity among variables is only a problem if these variables are included together in the same model, hence we should reevaluate collinearity more precisely when actual working models will be built. Here, we will simply identify which variables will be generally too problematic across likely models and, possibly, drop them (we will try to lower GVIF values as much as possible, considering that, even GVIF of 2 could still be problematic if ecological signals are weak; cf. Zuur *et al.*, 2010):

* The *geomem* and *geotex* variables are, without a surprise, problematic as they contain the same information. Therefore, we decide to only keep *geomem*.
* The *plantation* variable also seems highly problematic. Yet, as we have a strong interest in that variable, we choose to drop it **only temporarily**, only to detect other problematic variables. A particular attention to multicollinearity should be maintained when this variable will be included within models.
* Since our dataset contains sites on both sides of the Atlantic and since we do not have strong hypotheses regarding the effects of *longitude* (because our sites are located well within the longitudinal range of knotweeds), we decide to drop it.
* Since *degradation* contains most of the information contained in *pb_fixation* and *pb_durability*, we decide to drop it to reduce the number of variables. 
* As *coarse_env* is strongly positively correlated with *obstacles* and *slope*, we decide to drop it in order to reduce multicollinearity as well as the number of variables. 

After iteratively dropping these collinear variables and re-assessing GVIF values, we obtain a subset of potential explanatory variables with GVIF values globally < 2.5 and GVIF values will likely be even lower in later stages as models will not include so many variables at the same time. Naturally, we will also bear in mind the potential effects of the dropped variables in the discussion.

```{r redges final GVIF}
modvif <- stats::glm(lreg_edges~latitude+elevation+slope+flood+obstacles+stand_surface+geomem+maxveg+uprootexcav+distance+stripsoverlap_ok+tarpfix_multimethod+tarpfix_pierced+sedicover_height+trench_depth+followups+pb_fixation+pb_durability+tarping_duration+reg_elsewhere, data = redges, family = binomial)
#summary(modvif)

knitr::kable(car::vif(modvif))

redges %>% dplyr::select(-geotex, -longitude, -degradation, -coarse_env) -> redges

rm(modvif)
```
\
\
    
#### - Distributions and dispersion parameters
##### - Looking for outliers
\
We will look for potential outliers in the data using **boxplots** and **Cleveland dotplots** on our quantitative variables. 

```{r redges boxplot outliers, fig.align='center', fig.cap="**Figure 9.** Boxplots for all quantitative variables of the *regrowth at the edges of the tarped area* dataset", warning=FALSE}
jk.dusz.tarping::uni.boxplots(dataset = redges)
```

```{r redges dotplot, fig.align='center', fig.cap="**Figure 10.** Cleveland dotplots for all quantitative variables of the *regrowth at the edges of the tarped area* dataset"}
jk.dusz.tarping::uni.dotplots(dataset = redges)
```

By the look of these graphs, it is clear that we have many *extreme values* (possible outliers). Yet, extreme values are not necessarily **true outliers**:

* The *slope* variable looks fine.
* The *elevation*, *flood*, *obstacles*, *tarping_duration*, and *followups* variables may require mild transformations to be normalized but do not seem to contain too extreme values.
* The *distance*, *sedicover_height*, and *trench_depth* variables may require important transformations to be normalized. It would be interesting to see if their extreme values are too far from what could be expected from a Normal or a Poisson distribution (see below).
* The *latitude* and *stand_surface* variables contain problematic patterns/outliers.


```{r redges simu.distrib, include=FALSE, eval=FALSE}
simu.var <- dplyr::select(.data = redges, latitude, freq_monitoring, distance, sedicover_height, trench_depth)

jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "log-normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "poisson")
rm(simu.var)
```

After analysing random samples drawn from log-normal distributions (based on the parameters of our potentially problematic variables), we have good reason to believe that only *stand_surface* should be problematic. Consequently, we choose to delete the spotted outlier within *stand_surface* (i.e. the *stand_surface* of ca. 5000 m^2^).    
The other variables do not have distribution that a transformation could not fix, except perhaps *latitude* (we will keep an eye on the influence of its single extreme value). We will see later on whether we actually need such transformation (e.g log) or not. 


```{r redges update, include=FALSE}
redges <- redges[-(which.max(redges$stand_surface)),]
```
\
\
   
##### - Normality
\
We already got a pretty good idea of the distribution of our variables and not all statistical methods involve a Normality assumption (for instance, multiple linear regressions assume that residuals are normally distributed, not the variable themselves). Still, it's interesting and often useful to have a precise view of variables distributions, even for predictors and covariates. 

```{r redges histo, fig.align='center', fig.cap="**Figure 11.** Histograms for all quantitative variables of the *regrowth at the edges of the tarped area* dataset"}
uni.histograms(dataset = redges)
```
\
\

#### - Bivariate relationships
##### - Relationships among variables
\
The next step in our data exploration is to plot the response variables against each predictors and covariates in order to assess the **linearity** and **homoscedasticity** assumptions as well as identify potential **interactions** and **multivariate outliers**. It would be interesting to plot all variables against each other, but we have too many potential predictors and covariates for that.

```{r redges bivariate plot, fig.align='center', fig.cap="**Figure 12.** Multivariate representation of the *lreg_edges* against each potential explanatory variable", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
### To plot a response variable against all predictors/covariates, we will have to do use a for loop or the facet_wrap function of ggplot (or equivalent).
# For the "lreg_edges" variable (bounded continuous variable):
redges %>% dplyr::select(-manager_id, -xp_id) %>%
  dplyr::mutate_if(is.factor, as.numfactor) %>% 
  tidyr::pivot_longer(-c(lreg_edges), # Means that all columns of `mydata` need to be reshaped except "lreg_edges" & "fully_tarped" (works as well with the form `!lreg_edges`, and we can place it somewhere else: actually, it is the `cols =` argument)! We have to work in the long-format because that's how the tidyverse and thus ggplot2 love to work...
                               names_to = "Predictor", # Gives the name of the new grouping variable while "values_to" gives the name of the variable that will be created from the data stored in the cell value.
                               values_to = "Value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = Value, y = lreg_edges)) +
  ggplot2::facet_wrap(~Predictor, scales = "free_x", strip.position = "bottom") +
  ggplot2::geom_point(size = 1.5, alpha = 0.8, col = "palevioletred4") +
  ggplot2::scale_y_continuous(limits=c(0,1)) +
  ggplot2::geom_smooth(method = "lm", na.rm = TRUE, size = 1, col = "plum1") + 
  ggplot2::labs(x = "Predictor value", y = "Regrowth on the edges") +
  ggthemes::theme_hc(style = "darkunica") +
  ggplot2::theme(panel.border = ggplot2::element_blank(),
          legend.key = ggplot2::element_blank(),
          strip.background = ggplot2::element_blank(),
          strip.text = ggplot2::element_text(colour = "gray60", size = 10),
          strip.placement = "outside",
          axis.line.x = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          axis.line.y = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          legend.position = "right")
```

Various interesting observations can be made from these scatterplots:

* First, heteroscedasticity will perhaps not be surprising in our models as the variances of many variables do not seem to be homogeneous. On the other hand, we do not see clear multivariate outliers.
* Second, *lreg_edges* seems to increase with increasing *followups*, *freq_monitoring*, *geomem*, *obstacles* and *stripsoverlap_ok*. If the relationship with *obstacles* seems quite straightforward (and was one of our hypotheses), the other relationships are more surprising.
* Third, conversely and as hypothesized, *lreg_edges* seems to decrease with increasing *distance*, *sedicover_height*, *trench depth*, *tarping_duration* and, more surprisingly, with *stand surface*.

```{r redges export, include=FALSE}
### Finally, I export the dataset to be used for modelling in my "mydata" folder:
readr::write_csv(x = redges, file = here::here("mydata", "redges.csv"), append = FALSE)
```
\
\

******

### - For the response variables indicating knotweed regrowths within the tarped area (i.e. *lreg_tarpedarea*)
\
The first thing to do here is to prepare the sub-dataset that will be used to model the responses of the *lreg_tarpedarea* variable. To do so, the following steps have been performed:

* We selected the subset of predictors and covariates that will be used to model *lreg_tarpedarea*;
* Finally, we imputed missing values using **Random Forest** and the `missForest` package (Stekhoven & Buehlmann, 2012). 

The table below presents the *out-of-bag (OOB) imputation error* estimates for each type of variables (computed as the Normalized Root Mean Squared Error (NRMSE) for the numeric variables, and Proportion of Falsely Classified entries (PFC) for the categorical variables). OOB values near 1 indicate that imputations are not reliable, while values close to 0 indicate very low imputation error. For the meaning of the variables, please refer to the attached **documentation**.

```{r rtarped import & imput, include=FALSE}
rtarped <- jk.dusz.tarping::model_datasets(response.var = "tarpedarea")
rtarped <- dplyr::mutate(.data = rtarped, 
                        lreg_tarpedarea = as.factor(x = lreg_tarpedarea),
                        uprootexcav = as.factor(x = uprootexcav))

### Imputation of missing values using `missForest` (we could also have worked with `MICE`):
rtarped %>% dplyr::select(-manager_id, -xp_id) %>% as.data.frame() -> rtarped_mis # missForest only accepts data.frames or matrices (not tibbles). Variables must also be numeric or factors only (no character, date, difftime... classes)!

imput <- missForest::missForest(xmis = rtarped_mis, verbose = TRUE)
```

```{r rtarped imput error table, include=TRUE, message=FALSE, comment=NA}
knitr::kable(imput$OOBerror)
```

Now, let's take a quick look at our dataset.

```{r rtarped summary, include=TRUE, message=FALSE, comment=NA}
rtarped[,3:ncol(rtarped)] <- imput$ximp # Replacing the original dataset with the imputed one!
summary(rtarped)

rm(rtarped_mis, imput)
```
\
\

#### - Investigation of correlation structures
##### - Multivariate synthesis of environmental variables
\
We performed a **normed-PCA** on some of our environmental variables for which we do not have strong hypotheses (i.e. *difficulty_access*, *shade*, *forest*, *ruggedness*, *granulometry*) but we kept *slope*, *obstacles*, and *flood* because we suspected they might have a strong influence on the presence of knotweed regrowths. 

```{r rtarped env PCAs, warning=FALSE, fig.align='center', fig.cap="**Figure 13.** Correlation plot of the normed-PCA on environmental variables for the *regrowth within the tarped area* dataset"}
xxx <- dplyr::select(.data = rtarped, c(difficulty_access, shade, forest, ruggedness, granulometry))

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a fairly large amount of the variance of my 5 variables, I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
rtarped %>% dplyr::select(-difficulty_access, -shade, -forest, -ruggedness) %>%
  dplyr::rename(coarse_env = "granulometry") -> rtarped
rtarped$coarse_env <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.
```

As the first axis (PC) of our PCA satisfactorily synthesizes a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of the 5 environmental variables, we will use operations' coordinates on this axis to build a synthetic variable to summarize their environment. This new variable will be named **coarse_env** and will thus replace the other 5 variables in the dataset. Operations with *positive values* for this variable are located in sites that tend to be forested (and thus shaded), difficult to access, with uneven ground and a coarse soil texture (and vice-versa).

```{r rtarped followups, warning=FALSE, fig.align='center', fig.cap="**Figure 1 bis.** Correlation plot of the normed-PCA on follow-up variables for the *tarping efficiency* dataset"}
rtarped %>% dplyr::select(freq_monitoring, repairs, add_control) %>%
  dplyr::mutate(repairs = as.numfactor(x = repairs),
                add_control = as.numfactor(x = add_control))-> xxx

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a fairly large amount of the variance of my 5 variables, I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
rtarped %>% dplyr::select(-freq_monitoring, -repairs) %>%
  dplyr::rename(followups = "add_control") -> rtarped
rtarped$followups <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.
```

As the first axis (PC) of our PCA satisfactorily synthesizes a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of the 3 follow-up variables, we will use operations' coordinates on this axis to build a synthetic variable to summarize their follow-up. This new variable will be named **followups** and will thus replace the other 3 variables in the dataset. High *positive values* for this variable represent operations that were frequently monitored and where additional control and repairs were performed (and vice-versa).
\
\
  
##### - Correlation among variables
\
To investigate potential correlations and colinearity among our variables, we will start by computing a **correlation matrix** of the data. As most of our factors are either ordered factors (i.e. ordinal variables) or binary factors (i.e. boolean), we can use them in a correlation matrix. To do that however, computations will be based on *Spearman*'s rank correlation coefficient.  
To avoid giving too much weight to our own tarping experiments (those performed in Chalon s/ Saone with the French national railway company), we removed them from the data to compute correlations: their influence will be accounted for in further analyses by the use of **mixed-effect models**.

```{r rtarped corrplot, fig.align='center', fig.cap="**Figure 14.** Correlation matrix displaying *Spearman*'s $\\rho$ for the numeric variables and ordered factors of the *regrowth within the tarped area* dataset. Only significant correlations are displayed (with $\\alpha$ = 0.05)", warning=FALSE}
rm(res.pca, xxx, ttt)

# To convert all ordered factors into numeric variables usable in a correlation matrix:
rtarped %>% dplyr::filter(!manager_id == 1) %>% # We remove our own experiments as they share many features that will strongly influence correlations (it should not be a problem for modelling as we will use mixed-effect models).
  dplyr::select(-manager_id, -xp_id) %>%
  dplyr::mutate_if(.predicate = is.factor, .funs = as.numfactor) -> num.rtarped # as.numfactor() is my own custom function (see '01_data_cleaning.R').



# To compute the correlation matrix:
res.cor.rtarped <- round(stats::cor(num.rtarped, use = "complete.obs", method = "spearman"), 2)
# To compute a matrix of correlation p-values:
res.pcor.rtarped <- ggcorrplot::cor_pmat(x = num.rtarped, method = "spearman")

ggcorrplot::ggcorrplot(res.cor.rtarped, type = "upper",
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"), p.mat = res.pcor.rtarped, insig = "blank")

rm(num.rtarped, res.cor.rtarped, res.pcor.rtarped)
```

Many interesting patterns can be observed in this correlation matrix. Among them, we can see that:

* The *lreg_tarpedarea* variable seems to be positively correlated with *stripsoverlap_ok*, and all the *problem* variables which makes quite sense, except for the former.
* *geomem* is extremely negatively correlated with *geotex* and *woven_geotex*, which is logical since the use of geomembranes usually precludes the use of geotextiles (except in mixed settings).
* *tarpfix_multimethod* seems to be strongly positively correlated with *trench_depth*, *tarpfix_pierced*, *sedicover_height* and *plantation*, which is logical as trenches, staples, sediment covers and plantations are considered strong fabric fixation methods. 
* *levelling* is also correlated with *uprootexcav*, here again quite logically as the ground is often levelled after knotweed excavation. 
* Without surprise, *pierced_tarpinstall* seems to be strongly positively correlated with *obstacles* and *pb_durability*.
* *stand_surface* seems positively correlated with *flood*, *geotex*, *woven_geotex*, *distance* and *pb_fixation*.

It is worth reminding that accounting for multivariate relationships, interactions and the effect of our tarping operations (dropped for the current correlation matrix) will certainly give very different patterns.
\
\
   
##### - Multicollinearity
\
As multicollinearity is one of the biggest issues for inference modelling, we need to assess which explanatory variables and covariates are collinear and will possibly become problematic for modelling. To do so, our intention was to compute *Variance Inflation Factors* (VIF) for each variable. However, as several variables are categorical and as we will not be working with models fitted with ordinary least squares, we will rather use *Generalized VIF* (Fox & Monette, 1992).

```{r rtarped init GVIF}
modvif <- stats::glm(lreg_tarpedarea~latitude+longitude+elevation+slope+flood+coarse_env+obstacles+stand_surface+geomem+geotex+woven_geotex+maxveg+uprootexcav+levelling+fully_tarped+distance+stripsoverlap_ok+tarpfix_multimethod+tarpfix_pierced+pierced_tarpinstall+sedicover_height+trench_depth+plantation+followups+pb_fixation+pb_durability+pb_trampiercing+reg_edges+tarping_duration, data = rtarped, family = binomial)
#summary(modvif)

knitr::kable(car::vif(modvif))
```

As one could expect, there is evidence for **multicollinearity** among our 31 potential explanatory variables. However, strong collinearity among variables is only a problem if these variables are included together in the same model, hence we should reevaluate collinearity more precisely when actual working models will be built. Here, we will simply identify which variables will be generally too problematic across likely models and, possibly, drop them (we will try to lower GVIF values as much as possible, considering that, even GVIF of 2 could still be problematic if ecological signals are weak; cf. Zuur *et al.*, 2010):

* The *geomem*, *geotex*, and *woven_geotex* variables are, without a surprise, problematic as they contain the same information. Therefore, we decide to drop *geotex*.
* The *plantation* variable also seems highly problematic. Yet, as we have a strong interest in that variable, we choose to drop it **only temporarily**, only to detect other problematic variables. A particular attention to multicollinearity should be maintained when this variable will be included within models.
* Since our dataset contains sites on both sides of the Atlantic and since we do not have strong hypotheses regarding the effects of *longitude* (because our sites are located well within the longitudinal range of knotweeds), we decide to drop it.
* As much of the information contained in *obstacles* is shared with *pierced_tarpinstall* and *coarse_env*, we decide to drop it in order to reduce multicollinearity as well as the number of variables. 

After iteratively dropping these collinear variables and re-assessing GVIF values, we obtain a subset of potential explanatory variables with GVIF values globally < 2.5 and GVIF values will likely be even lower in later stages as models will not include so many variables at the same time. Naturally, we will also bear in mind the potential effects of the dropped variables in the discussion.

```{r rtarped final GVIF}
modvif <- stats::glm(lreg_tarpedarea~latitude+elevation+slope+flood+coarse_env+stand_surface+geomem+woven_geotex+maxveg+uprootexcav+levelling+fully_tarped+distance+stripsoverlap_ok+tarpfix_multimethod+tarpfix_pierced+pierced_tarpinstall+sedicover_height+trench_depth+followups+pb_fixation+pb_durability+pb_trampiercing+reg_edges+tarping_duration, data = rtarped, family = binomial)
#summary(modvif)

knitr::kable(car::vif(modvif))

rtarped %>% dplyr::select(-geotex, -longitude, -obstacles) -> rtarped

rm(modvif)
```
\
\
    
#### - Distributions and dispersion parameters
##### - Looking for outliers
\
We will look for potential outliers in the data using **boxplots** and **Cleveland dotplots** on our quantitative variables. 

```{r rtarped boxplot outliers, fig.align='center', fig.cap="**Figure 15.** Boxplots for all quantitative variables of the *regrowth within the tarped area* dataset", warning=FALSE}
jk.dusz.tarping::uni.boxplots(dataset = rtarped)
```

```{r rtarped dotplot, fig.align='center', fig.cap="**Figure 16.** Cleveland dotplots for all quantitative variables of the *regrowth within the tarped area* dataset"}
jk.dusz.tarping::uni.dotplots(dataset = rtarped)
```

By the look of these graphs, it is clear that we have many *extreme values* (possible outliers). Yet, extreme values are not necessarily **true outliers**:

* The *slope* and *coarse_env* variables look fine.
* The *elevation*, *flood*, and *tarping_duration* variables may require mild transformations to be normalized but do not seem to contain too extreme values.
* The *distance*, *pierced_tarpinstall*, *sedicover_height*, *trench_depth*, and *followups* variables may require important transformations to be normalized. It would be interesting to see if their extreme values are too far from what could be expected from a Normal or a Poisson distribution (see below).
* The *latitude* and *stand_surface* variables contain problematic patterns/outliers.


```{r rtarped simu.distrib, include=FALSE, eval=FALSE}
simu.var <- dplyr::select(.data = rtarped, latitude, freq_monitoring, distance, sedicover_height, trench_depth)

jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "log-normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "poisson")
rm(simu.var)
```

After analysing random samples drawn from log-normal distributions (based on the parameters of our potentially problematic variables), we have good reason to believe that only *stand_surface* should be problematic. Consequently, we choose to delete the spotted outlier within *stand_surface* (i.e. the *stand_surface* of ca. 5000 m^2^).  
The other variables do not have distribution that a transformation could not fix, except perhaps *latitude* (we will keep an eye on the influence of its single extreme value). We will see later on whether we actually need such transformation (e.g log) or not. 

```{r rtarped update, include=FALSE}
rtarped <- rtarped[-(which.max(rtarped$stand_surface)),]
```
\
\
   
##### - Normality
\
We already got a pretty good idea of the distribution of our variables and not all statistical methods involve a Normality assumption (for instance, multiple linear regressions assume that residuals are normally distributed, not the variable themselves). Still, it's interesting and often useful to have a precise view of variables distributions, even for predictors and covariates. 

```{r rtarped histo, fig.align='center', fig.cap="**Figure 17.** Histograms for all quantitative variables of the *regrowth within the tarped area* dataset"}
uni.histograms(dataset = rtarped)
```
\
\

#### - Bivariate relationships
##### - Relationships among variables
\
The next step in our data exploration is to plot the response variables against each predictors and covariates in order to assess the **linearity** and **homoscedasticity** assumptions as well as identify potential **interactions** and **multivariate outliers**. It would be interesting to plot all variables against each other, but we have too many potential predictors and covariates for that.

```{r rtarped bivariate plot, fig.align='center', fig.cap="**Figure 18.** Multivariate representation of *lreg_tarpedarea* against each potential explanatory variable", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
### To plot a response variable against all predictors/covariates, we will have to do use a for loop or the facet_wrap function of ggplot (or equivalent).
# For the "lreg_tarpedarea" variable (bounded continuous variable):
rtarped %>% dplyr::select(-manager_id, -xp_id) %>%
  dplyr::mutate_if(is.factor, as.numfactor) %>% 
  tidyr::pivot_longer(-c(lreg_tarpedarea), # Means that all columns of `mydata` need to be reshaped except "lreg_tarpedarea" & "fully_tarped" (works as well with the form `!lreg_tarpedarea`, and we can place it somewhere else: actually, it is the `cols =` argument)! We have to work in the long-format because that's how the tidyverse and thus ggplot2 love to work...
                               names_to = "Predictor", # Gives the name of the new grouping variable while "values_to" gives the name of the variable that will be created from the data stored in the cell value.
                               values_to = "Value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = Value, y = lreg_tarpedarea)) +
  ggplot2::facet_wrap(~Predictor, scales = "free_x", strip.position = "bottom") +
  ggplot2::geom_point(size = 1.5, alpha = 0.8, col = "palevioletred4") +
  ggplot2::scale_y_continuous(limits=c(0,1)) +
  ggplot2::geom_smooth(method = "lm", na.rm = TRUE, size = 1, col = "plum1") + 
  ggplot2::labs(x = "Predictor value", y = "Regrowth within the tarped area") +
  ggthemes::theme_hc(style = "darkunica") +
  ggplot2::theme(panel.border = ggplot2::element_blank(),
          legend.key = ggplot2::element_blank(),
          strip.background = ggplot2::element_blank(),
          strip.text = ggplot2::element_text(colour = "gray60", size = 10),
          strip.placement = "outside",
          axis.line.x = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          axis.line.y = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          legend.position = "right")
```

Various interesting observations can be made from these scatterplots:

* First, heteroscedasticity will perhaps not be surprising in our models as the variances of many variables do not seem to be homogeneous. On the other hand, we do not see clear multivariate outliers.
* Second, *lreg_tarpedarea* seems to increase with increasing *followups*, *flood*, *maxveg*, all *problem variables*, *stand-surface*, *stripsoverlap_ok* and *woven_geotextile*. If some relationships seem quite straightforward (and are in accordance with our hypotheses), others are more surprising and may just be noise. 
* Third, *lreg_tarpedarea* seems to decrease with increasing *elevation*, *geomem*,and *pierced_tarpinstall*. If the first two relationships seem quite straightforward (and are in accordance with our hypotheses), the third one is more surprising!

```{r rtarped export, include=FALSE}
### Finally, I export the dataset to be used for modelling in my "mydata" folder:
readr::write_csv(x = rtarped, file = here::here("mydata", "rtarped.csv"), append = FALSE)
```
\
\

******

### - For the response variables indicating knotweed regrowths at fabric strip overlaps (i.e. *reg_stripsoverlap*)
\
The first thing to do here is to prepare the sub-dataset that will be used to model the responses of the *reg_stripsoverlap* variable. To do so, the following steps have been performed:

* We selected the subset of predictors and covariates that will be used to model *reg_stripsoverlap*;
* We selected observations that had no missing values for *reg_stripsoverlap* and for *strips_overlap* as the first variable is our response value and the second is central for our hypotheses;
* Finally, we imputed missing values using **Random Forest** and the `missForest` package (Stekhoven & Buehlmann, 2012). 

The table below presents the *out-of-bag (OOB) imputation error* estimates for each type of variables (computed as the Normalized Root Mean Squared Error (NRMSE) for the numeric variables, and Proportion of Falsely Classified entries (PFC) for the categorical variables). OOB values near 1 indicate that imputations are not reliable, while values close to 0 indicate very low imputation error. For the meaning of the variables, please refer to the attached **documentation**.

```{r roverlaps import & imput, include=FALSE}
roverlaps <- jk.dusz.tarping::model_datasets(response.var = "overlaps")
roverlaps <- dplyr::filter(.data = roverlaps, reg_stripsoverlap != "NA") %>%
  dplyr::filter(strips_overlap != "NA")

### Imputation of missing values using `missForest` (we could also have worked with `MICE`):
roverlaps %>% dplyr::select(-manager_id, -xp_id) %>% as.data.frame() -> roverlaps_mis # missForest only accepts data.frames or matrices (not tibbles). Variables must also be numeric or factors only (no character, date, difftime... classes)!

imput <- missForest::missForest(xmis = roverlaps_mis, verbose = TRUE)
```

```{r roverlaps imput error table, include=TRUE, message=FALSE, comment=NA}
knitr::kable(imput$OOBerror)
```

Now, let's take a quick look at our dataset.

```{r roverlaps summary, include=TRUE, message=FALSE, comment=NA}
roverlaps[,3:ncol(roverlaps)] <- imput$ximp # Replacing the original dataset with the imputed one!
summary(roverlaps)

rm(roverlaps_mis, imput)
```
\
\

#### - Investigation of correlation structures
##### - Multivariate synthesis of environmental variables
\
We performed a **normed-PCA** on some of our environmental variables for which we do not have strong hypotheses (i.e. *difficulty_access*, *shade*, *forest*, *ruggedness*, *granulometry*) but we kept *slope*, *obstacles*, and *flood* because we suspected they might have a strong influence on the presence of knotweed regrowths. 

```{r roverlaps env PCAs, warning=FALSE, fig.align='center', fig.cap="**Figure 19.** Correlation plot of the normed-PCA on environmental variables for the *regrowth at strips overlap* dataset"}
xxx <- dplyr::select(.data = roverlaps, c(difficulty_access, shade, forest, ruggedness, granulometry))

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a fairly large amount of the variance of my 5 variables, I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
roverlaps %>% dplyr::select(-difficulty_access, -shade, -forest, -ruggedness) %>%
  dplyr::rename(coarse_env = "granulometry") -> roverlaps
roverlaps$coarse_env <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.
```

As the first axis (PC) of our PCA satisfactorily synthesizes a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of the 5 environmental variables, we will use operations' coordinates on this axis to build a synthetic variable to summarize their environment. This new variable will be named **coarse_env** and will thus replace the other 5 variables in the dataset. Operations with *positive values* for this variable are located in sites that tend to be forested (and thus shaded), difficult to access, with uneven ground and a coarse soil texture (and vice-versa).

```{r roverlaps followups, warning=FALSE, fig.align='center', fig.cap="**Figure 1 bis.** Correlation plot of the normed-PCA on follow-up variables for the *tarping efficiency* dataset"}
roverlaps %>% dplyr::select(freq_monitoring, repairs, add_control) %>%
  dplyr::mutate(repairs = as.numfactor(x = repairs),
                add_control = as.numfactor(x = add_control))-> xxx

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a fairly large amount of the variance of my 5 variables, I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
roverlaps %>% dplyr::select(-freq_monitoring, -repairs) %>%
  dplyr::rename(followups = "add_control") -> roverlaps
roverlaps$followups <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.
```

As the first axis (PC) of our PCA satisfactorily synthesizes a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of the 3 follow-up variables, we will use operations' coordinates on this axis to build a synthetic variable to summarize their follow-up. This new variable will be named **followups** and will thus replace the other 3 variables in the dataset. High *positive values* for this variable represent operations that were frequently monitored and where additional control and repairs were performed (and vice-versa).
\
\
  
##### - Correlation among variables
\
To investigate potential correlations and colinearity among our variables, we will start by computing a **correlation matrix** of the data. As most of our factors are either ordered factors (i.e. ordinal variables) or binary factors (i.e. boolean), we can use them in a correlation matrix. To do that however, computations will be based on *Spearman*'s rank correlation coefficient.  
To avoid giving too much weight to our own tarping experiments (those performed in Chalon s/ Saone with the French national railway company), we removed them from the data to compute correlations: their influence will be accounted for in further analyses by the use of **mixed-effect models**.

```{r roverlaps corrplot, fig.align='center', fig.cap="**Figure 20.** Correlation matrix displaying *Spearman*'s $\\rho$ for the numeric variables and ordered factors of the *regrowth at strips overlap* dataset. Only significant correlations are displayed (with $\\alpha$ = 0.05)", warning=FALSE}
rm(res.pca, xxx, ttt)

# To convert all ordered factors into numeric variables usable in a correlation matrix:
roverlaps %>% dplyr::filter(!manager_id == 1) %>% # We remove our own experiments as they share many features that will strongly influence correlations (it should not be a problem for modelling as we will use mixed-effect models).
  dplyr::select(-manager_id, -xp_id) %>%
  dplyr::mutate_if(.predicate = is.factor, .funs = as.numfactor) -> num.roverlaps # as.numfactor() is my own custom function (see '01_data_cleaning.R').



# To compute the correlation matrix:
res.cor.roverlaps <- round(stats::cor(num.roverlaps, use = "complete.obs", method = "spearman"), 2)
# To compute a matrix of correlation p-values:
res.pcor.roverlaps <- ggcorrplot::cor_pmat(x = num.roverlaps, method = "spearman")

ggcorrplot::ggcorrplot(res.cor.roverlaps, type = "upper",
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"), p.mat = res.pcor.roverlaps, insig = "blank")

rm(num.roverlaps, res.cor.roverlaps, res.pcor.roverlaps)
```

Many interesting patterns can be observed in this correlation matrix. Among them, we can see that:

* The *reg_stripsoverlap* variable seems to be mildly positively correlated with *obstacles*.
* As usual, *geomem* is extremely negatively correlated with *geotex*, which is logical since the use of geomembranes usually precludes the use of geotextiles (except in mixed settings).
* *stripfix_pierced* is strongly positively correlated with *tarpfix_pierced*, but also with *fully_tarped*.
* *stripfix_taped* is mildly positively correlated with *maxveg*, and negatively with *add_control*, and *reg_elsewhere*. 
* Conversely, *strips_overlap* is mildly negatively correlated with *pb_durability*. 
* *tarpfix_multimethod* seems to be positively correlated with *trench_depth* and *plantation*, which is logical.
* *levelling* is also correlated with *uprootexcav* and *trench_depth*, here again quite logically as the ground is often levelled after knotweed excavation or trench digging. 
* *obstacles* is, among other things, correlated with *coarse_env*, *distance*, *pb_durability* and *reg_elsewhere*.
* *reg_elsewhere* is also negatively correlated with *maxveg* and *trench_depth*, all these correlations seeming quite logical. 
* Finally, *stand_surface* seems positively correlated with *flood*, *geotex*, *plantation*, *tarpfix_pierced* and *pb_fixation*.

It is worth reminding that accounting for multivariate relationships, interactions and the effect of our tarping operations (dropped for the current correlation matrix) will certainly give very different patterns.
\
\
   
##### - Multicollinearity
\
As multicollinearity is one of the biggest issues for inference modelling, we need to assess which explanatory variables and covariates are collinear and will possibly become problematic for modelling. To do so, our intention was to compute *Variance Inflation Factors* (VIF) for each variable. However, as several variables are categorical and as we will not be working with models fitted with ordinary least squares, we will rather use *Generalized VIF* (Fox & Monette, 1992).  
Because of convergence problems, we had to remove some variables before computing GVIF values. So here is the list of the dropped variables: *longitude*, *geotex* and *tarpfix_pierced* (because it was redundant with *stripfix_pierced*).

```{r roverlaps init GVIF}
modvif <- stats::glm(reg_stripsoverlap~latitude+elevation+slope+flood+coarse_env+obstacles+stand_surface+geomem+maxveg+uprootexcav+levelling+fully_tarped+distance+strips_overlap+tarpfix_multimethod+stripfix_pierced+stripfix_taped+sedicover_height+trench_depth+plantation+followups+pb_fixation+pb_durability+reg_elsewhere+tarping_duration, data = roverlaps, family = binomial)
#summary(modvif)

knitr::kable(car::vif(modvif))
```

As one could expect, there is evidence for **multicollinearity** among our 27 potential explanatory variables. However, strong collinearity among variables is only a problem if these variables are included together in the same model, hence we should reevaluate collinearity more precisely when actual working models will be built. Here, we will simply identify which variables will be generally too problematic across likely models and, possibly, drop them (we will try to lower GVIF values as much as possible, considering that, even GVIF of 2 could still be problematic if ecological signals are weak; cf. Zuur *et al.*, 2010):

* As *levelling* is highly correlated with *uprootexcav* and seems to be the most problematic variable in terms of multicollinearity, we decide to drop *uprootexcav*.
* As we do not have a strong hypothesis regarding the effect of *floods* on the probability of finding regrowths at strips overlap, we decide to drop the *flood* variable and, as much of the information contained in *obstacles* is also contained in *coarse_env*, we decide to drop it as well.
* Finally, we decide to drop *maxveg* for similar reasons.

After iteratively dropping these collinear variables and re-assessing GVIF values, we obtain a subset of potential explanatory variables with GVIF values globally < 2.6 and GVIF values will likely be even lower in later stages as models will not include so many variables at the same time. Naturally, we will also bear in mind the potential effects of the dropped variables in the discussion.

```{r roverlaps final GVIF}
modvif <- stats::glm(reg_stripsoverlap~latitude+elevation+slope+coarse_env+stand_surface+geomem+levelling+fully_tarped+distance+strips_overlap+tarpfix_multimethod+stripfix_pierced+stripfix_taped+sedicover_height+trench_depth+plantation+followups+pb_fixation+pb_durability+reg_elsewhere+tarping_duration, data = roverlaps, family = binomial)
#summary(modvif)

knitr::kable(car::vif(modvif))

roverlaps %>% dplyr::select(-geotex, -longitude, -flood, -obstacles, -tarpfix_pierced, -uprootexcav, -maxveg) -> roverlaps

rm(modvif)
```
\
\
    
#### - Distributions and dispersion parameters
##### - Looking for outliers
\
We will look for potential outliers in the data using **boxplots** and **Cleveland dotplots** on our quantitative variables. 

```{r roverlaps boxplot outliers, fig.align='center', fig.cap="**Figure 21.** Boxplots for all quantitative variables of the *regrowth at strips overlap* dataset", warning=FALSE}
jk.dusz.tarping::uni.boxplots(dataset = roverlaps)
```

```{r roverlaps dotplot, fig.align='center', fig.cap="**Figure 22.** Cleveland dotplots for all quantitative variables of the *regrowth at strips overlap* dataset"}
jk.dusz.tarping::uni.dotplots(dataset = roverlaps)
```

By the look of these graphs, it is clear that we have many *extreme values* (possible outliers). Yet, extreme values are not necessarily **true outliers**:

* The *slope* and *coarse_env* variables look fine.
* The *latitude*, *elevation*, *strips_overlap* and *tarping_duration* variables may require mild transformations to be normalized but do not seem to contain too extreme values.
* The *distance*, *sedicover_height*, *trench_depth*, and *followups* variables may require important transformations to be normalized. 
* The *stand_surface* variable contains a problematic pattern/outlier.


```{r roverlaps simu.distrib, include=FALSE, eval=FALSE}
simu.var <- dplyr::select(.data = roverlaps, latitude, freq_monitoring, distance, sedicover_height, trench_depth)

jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "log-normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "poisson")
rm(simu.var)
```

After analysing random samples drawn from log-normal distributions (based on the parameters of our potentially problematic variables), we have good reason to believe that only *stand_surface* should be problematic. Consequently, we choose to delete the spotted outlier within *stand_surface* (i.e. the *stand_surface* of ca. 5000 m^2^).  

```{r roverlaps update, include=FALSE}
roverlaps <- roverlaps[-(which.max(roverlaps$stand_surface)),]
```
\
\
   
##### - Normality
\
We already got a pretty good idea of the distribution of our variables and not all statistical methods involve a Normality assumption (for instance, multiple linear regressions assume that residuals are normally distributed, not the variable themselves). Still, it's interesting and often useful to have a precise view of variables distributions, even for predictors and covariates. 

```{r roverlaps histo, fig.align='center', fig.cap="**Figure 23.** Histograms for all quantitative variables of the *regrowth at strips overlap* dataset"}
uni.histograms(dataset = roverlaps)
```
\
\

#### - Bivariate relationships
##### - Relationships among variables
\
The next step in our data exploration is to plot the response variables against each predictors and covariates in order to assess the **linearity** and **homoscedasticity** assumptions as well as identify potential **interactions** and **multivariate outliers**. It would be interesting to plot all variables against each other, but we have too many potential predictors and covariates for that.

```{r roverlaps bivariate plot, fig.align='center', fig.cap="**Figure 24.** Multivariate representation of *reg_stripsoverlap* against each potential explanatory variable", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
### To plot a response variable against all predictors/covariates, we will have to do use a for loop or the facet_wrap function of ggplot (or equivalent).
# For the "reg_stripsoverlap" variable (bounded continuous variable):
roverlaps %>% dplyr::select(-manager_id, -xp_id) %>%
  dplyr::mutate_if(is.factor, as.numfactor) %>% 
  tidyr::pivot_longer(-c(reg_stripsoverlap), # Means that all columns of `mydata` need to be reshaped except "reg_stripsoverlap" & "fully_tarped" (works as well with the form `!reg_stripsoverlap`, and we can place it somewhere else: actually, it is the `cols =` argument)! We have to work in the long-format because that's how the tidyverse and thus ggplot2 love to work...
                               names_to = "Predictor", # Gives the name of the new grouping variable while "values_to" gives the name of the variable that will be created from the data stored in the cell value.
                               values_to = "Value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = Value, y = reg_stripsoverlap)) +
  ggplot2::facet_wrap(~Predictor, scales = "free_x", strip.position = "bottom") +
  ggplot2::geom_point(size = 1.5, alpha = 0.8, col = "palevioletred4") +
  ggplot2::scale_y_continuous(limits=c(0,1)) +
  ggplot2::geom_smooth(method = "lm", na.rm = TRUE, size = 1, col = "plum1") + 
  ggplot2::labs(x = "Predictor value", y = "Regrowth at strips overlap") +
  ggthemes::theme_hc(style = "darkunica") +
  ggplot2::theme(panel.border = ggplot2::element_blank(),
          legend.key = ggplot2::element_blank(),
          strip.background = ggplot2::element_blank(),
          strip.text = ggplot2::element_text(colour = "gray60", size = 10),
          strip.placement = "outside",
          axis.line.x = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          axis.line.y = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          legend.position = "right")
```

Various interesting observations can be made from these scatterplots:

* First, heteroscedasticity will perhaps not be surprising in some of our models as the variances of several variables do not seem to be homogeneous. On the other hand, we do not see clear multivariate outliers.
* Second, *reg_stripsoverlap* seems to increase with increasing *followups*, *coarse_env*, *elevation*, *freq_monitoring*, *geomem*, *reg_elsewhere*, *sedicover_height*, and *slope*. If some relationships seem quite straightforward (and are in accordance with our hypotheses), others are more surprising and may just be noise. 
* Third, *reg_stripsoverlap* seems to decrease with increasing *distance*, *latitude*, *levelling*, *plantation*, the *strips* related variables, *tarping duration* and *trench_depth*. 

```{r roverlaps export, include=FALSE}
### Finally, I export the dataset to be used for modelling in my "mydata" folder:
readr::write_csv(x = roverlaps, file = here::here("mydata", "roverlaps.csv"), append = FALSE)
```
\
\

******


## Next steps

1) Set drake + ask questions SO/CV !!! + Conditional boxplot (homoschedasticity for the continuous Y)§§§§§§§ kurtosis + skewness + cond box + coplot + kernel density§§§§§§§§§§§ + TESTER si stand_surface change de coef quand /10 (en dizaine de m2)
2) Transformations and standardization (for scaling, especially important when I use interactions terms)
3) Write models!

# - References

A compléter!!!!!!!!!

Field, A. (2009). Discovering statistics using SPSS. London: SAGE.
Fox, J. (2008) Applied Regression Analysis and Generalized Linear Models, 2nd
edn. Sage Publications,CA.
Gravetter, F., & Wallnau, L. (2014). Essentials of statistics for the behavioral sciences (8th ed.). Belmont, CA: Wadsworth.
Montgomery, D.C. & Peck, E.A. (1992) Introduction to Linear Regression Analysis. Wiley,New York.
Martin et al 2019 alpine botany

