---
title: "Data Preparation Report - Knotweed's Tarping Survey (Dusz et al., 2020)"
author: "François-Marie Martin"
date: "11/27/2020"
output:
   html_document:
     number_sections: yes
     toc: yes
     toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# - Reproductibility
## - System information for my session  

```{r session information}
rm(list=ls())
sessionInfo()
```


  


# - Data visualisation and exploration  
## - Data exploration following Zuur *et al.* (2010)

```{r data import and summary, include=FALSE}
library(jk.dusz.tarping)
zzz_mydata_cleaned <- jk.dusz.tarping::clean_my_data()
```

In order to build valid and meaningful models to explain or predict the success/failure of tarping operations for the control of Japanese knotweed *s.l.*, we need first to learn more about our data. Therefore, we thoroughly examined our variables (e.g. outliers, distribution, interactions, colinearity, variance homogeneity, observation independence) mostly following the protocol presented by Zuur *et al.* (2010) for each of our **5 response variables**: *eff_eradication*, *efficiency*, *latest_reg_edges*, *reg_overlaps*, *reg_tarpedarea*.

As our global dataset contains many variables (*p* = 85), it would be interesting to reduce their number in order to spare time for further exploration and analyses. As such, the first steps of our data exploration for each response variable will aim at reducing the dimensionality of the data. First, we will try and see if the *environmental variables* can be coerced into a synthetic variable (e.g. through a PCA). Then, we will investigate correlations among variables as well as potential colinearity. This should have the twofold advantage of improving our understanding of the data and enabling us to remove colinear and too strongly correlated variables. Finally, we will carry on data exploration in various ways in order to detect outliers, identify distributions, heteroscedasticity, univariate and multivariate relationships...



### - For the response variables related to tarping *efficiency*

The first thing to do here is to prepare the sub-dataset that will be used to model the responses of the *efficiency variables* (i.e. **efficiency**, **eff_eradication** and **high_eff**). To do so, the following steps have been performed:

* We selected the subset of predictors and covariates that will be used to model tarping efficiency;
* We left out the observations that were too recent (installed less than 2 growing seasons ago) and those for which we could not obtain an *efficiency score* (i.e. NA);
* We deleted operations that got prematurely interrupted (e.g. because of vandalism) since there failure results from external causes.

Now, let's take a quick look at our dataset. For the meaning of the variables, please refer to the attached **documentation**.

```{r erad import, include=TRUE, message=FALSE, comment=NA}
erad <- jk.dusz.tarping::model_datasets(response.var = "efficiency")

erad %>% dplyr::select(-liner_geomem, -agri_geomem,-woven_geotex, -mulching_geotex, -pla_geotex, -other_unknown, -grammage, -age, -thickness, -pierced_tarpinstall, -add_control_type) %>%
  dplyr::filter(eff_eradication != "NA") %>%
  dplyr::filter(tarping_duration >= 2) %>%
  dplyr::filter(xp_id != "54") -> erad # Operation n°54 was destroyed because of vandalism.

summary(erad)
```



#### - Investigation of correlation structures
##### - Multivariate synthesis of environmental variables

We performed a **normed-PCA** on some of our environmental variables for which we do not have strong hypotheses (i.e. *difficulty_access*, *shade*, *forest*, *ruggedness*, *granulometry*) but we kept *slope*, *obstacles*, and *flood* because we suspected they might have a major influence on the success/failure of tarping. In order to get optimum results, we imputed missing values using the *Regularised Iterative PCA algorithm* (Josse & Husson, 2013) of the `missMDA` package (Josse & Husson, 2016). 

```{r erad env PCAs, warning=FALSE, fig.align='center', fig.cap="**Figure 1.** Correlation plot of the normed-PCA on environmental variables for the *tarping efficiency* dataset"}
xxx <- erad[,11:15]

# As there are missing values in some variables, we will impute them using the missPCA() function.
# But first, we need to estimate how many components are required to correctly impute the missing values:
nb <- missMDA::estim_ncpPCA(xxx, ncp.max=5) # Can be time consuming. Here, the best result is 0 but we will take 2 as it is (almost) the second best option.
# Then we impute the missing values and extract the imputed dataset:
res.imput <- missMDA::imputePCA(X = xxx, method = "Regularized", scale = TRUE, ncp = 2)
xxx <- res.imput$completeObs # Gives the completed dataset!

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a large amount of the variance of my 5 variables (38.2%), I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
erad %>% dplyr::select(-difficulty_access, -shade, -forest, -ruggedness) %>%
  dplyr::rename(coarse_env = "granulometry") -> erad
erad$coarse_env <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.

# To plot side by side:
#gridExtra::grid.arrange(FIG_1, FIG_2, ncol = 2)
```

As the first axis (PC) of our PCA satisfactorily synthesizes a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of the 5 environmental variables, we will use operations' coordinates on this axis to build a synthetic variable to summarize their environment. This new variable will be named **coarse_env** and will thus replace the other 5 variables in the dataset. Operations with *positive values* for this variable are located in sites that tend to be forested (and thus shaded), difficult to access, with uneven ground and a coarse soil texture (and vice-versa).


```{r, eval=FALSE, include=FALSE}
# PEUT ETRE INTERESSANT POUR D'AUTRES MODELES???????? mouais...
# pb <- zzz_mydata_cleaned[, c(59,60,64,66,68)] #PB+REG (2dim = 53%)
# pb <- na.omit(object = pb) # To have no obs. with NAs!
# summary(pb)
# 
# 
# #MCA
# acm <- FactoMineR::MCA(X = pb, ncp = 5,graph = FALSE)
# acm$eig
# plot(acm, label = "var")
# plot(acm, choix = "var")
```

  
##### - Correlation among variables

To investigate potential correlations and colinearity among our variables, we will start by computing a **correlation matrix** of the data. As most of our factors are either ordered factors (i.e. ordinal variables) or binary factors (i.e. boolean), we can use them in a correlation matrix. To do that however, computations will be based on *Spearman*'s rank correlation coefficient.

```{r erad corrplot, fig.align='center', fig.cap="**Figure 2.** Correlation matrix displaying *Spearman*'s $\\rho$ for the numeric variables and ordered factors of the *tarping efficiency* dataset. Only significant correlations are displayed (with $\\alpha$ = 0.05)", warning=FALSE}
rm(res.pca, res.imput, xxx, ttt, nb)

# To convert all ordered factors into numeric variables usable in a correlation matrix:
erad %>% dplyr::select(-xp_id, -goals) %>%
  dplyr::mutate_if(.predicate = is.factor, .funs = as.numfactor) -> num.erad # as.numfactor is my own custom function (see '01_data_cleaning.R').
#num.erad <- erad[, sapply(erad, is.numeric)] # Select only numeric columns.


# To compute the correlation matrix:
res.cor.erad <- round(stats::cor(num.erad, use = "complete.obs", method = "spearman"), 2)
# To compute a matrix of correlation p-values:
res.pcor.erad <- ggcorrplot::cor_pmat(x = num.erad, method = "spearman")

ggcorrplot::ggcorrplot(res.cor.erad, type = "upper",
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"), p.mat = res.pcor.erad, insig = "blank")

rm(num.erad, res.cor.erad, res.pcor.erad)
```

Many interesting patterns can be observed in this correlation matrix. Among them, we can see that:

* One or several of the *efficiency* variables seem to be mildly positively correlated with *distance*, and *fully_tarped* and negatively with *obstacles*, *add_control* and *pb_durability*. 
* *add_control*, *rapairs* and *plantation* display a relatively similar pattern of positive correlation with *slope*, *coarse_env*, *flood* and *geomem*, and negative correlation with *geotex*, *weedsp_geotex* and *maxveg*. However, this pattern certainly bears the mark of our own tarping experiments in Chalon that were installed in June and where we did not repaired set ups nor performed additional controls nor planted anything (most correlations with *weedsp_geotex* should also be influenced by our experiments).
* *pb_fixation* seems to be strongly positively correlated with *stand_surface* and, to a lesser extent, with *tarpfix_pierced*.
* *stand_surface* seems to be strongly positively correlated with *flood*.

It is worth reminding that these correlations are just correlations (not **causation**) and only bivariate. Accounting for multivariate relationships and interactions will certainly give very different patterns.


   
##### - Multicollinearity

As multicollinearity is one of the biggest issues for inference modelling, we need to assess which explanatory variables and covariates are collinear and will possibly become problematic for modelling. To do so, our intention was to compute *Variance Inflation Factors* (VIF) for each variable. However, as several variables are categorical and as we will not be working with models fitted with ordinary least squares, we will rather use *Generalized VIF* (Fox & Monette, 1992).

```{r erad init GVIF}
erad %>% dplyr::select(-xp_id, -goals, -eff_eradication, -high_eff) -> mydata

modvif <- stats::glm((efficiency/10)~latitude+longitude+elevation+freq_monitoring+slope+coarse_env+obstacles+flood+geomem+geotex+weedsp_geotex+maxveg+uprootexcav+stand_surface+fully_tarped+distance+tarping_duration+stripsoverlap_ok+tarpfix_multimethod+tarpfix_pierced+sedicover_height+trench_depth+plantation+repairs+add_control+degradation+pb_fixation+pb_durability, data = mydata, family = quasibinomial)
#summary(modvif)

knitr::kable(car::vif(modvif))
```

As one could expect, there is evidence for important **multicollinearity** among our 26 potential explanatory variables. However, strong collinearity among variables is only a problem if these variables are included together in the same model, hence the necessity to reevaluate collinearity more precisely when actual working models will be built. Here, we will simply identify which variables will be generally too problematic across likely models and, possibly, drop them (we will try to lower GVIF values as much as possible, considering that, even GVIF of 2 could still be problematic if ecological signals are weak; cf. Zuur *et al.*, 2010):

* The variables *geomem* and *geotex* (and *weedsp_geotex*) are, without a surprise, very problematic as they contain the same information. Therefore, we decide to only keep *geomem*.
* The *plantation* variable also seems highly problematic. Yet, as we have a strong interest in that variable, we choose to drop it **only temporarily**, only to detect other problematic variables. A particular attention to multicollinearity should be maintained when this variable will be included within models.
* Since our dataset contains sites on both sides of the Atlantic and since we do not have strong hypotheses regarding the effects of *longitude* (because our sites are located well within the longitudinal range of knotweeds), we decide to drop it.
* Since *degradation* contains most of the information contained in *pb_fixation* and *pb_durability*, we decide to drop it to reduce the number of variables. 

After iteratively dropping these strongly collinear variables and re-assessing GVIF values, we obtain a subset of potential explanatory variables with GVIF values globally < 3.5 and GVIF values will likely be even lower in later stages as models will not include so many variables at the same time. Naturally, we will also bear in mind the potential effects of the dropped variables in the discussion.

```{r erad final GVIF}
modvif <- stats::glm((efficiency/10)~latitude+elevation+freq_monitoring+slope+coarse_env+flood+obstacles+geomem+maxveg+uprootexcav+stand_surface+fully_tarped+distance+tarping_duration+stripsoverlap_ok+tarpfix_multimethod+tarpfix_pierced+sedicover_height+trench_depth+repairs+add_control+pb_fixation+pb_durability, data = mydata, family = quasibinomial)
#summary(modvif)

knitr::kable(car::vif(modvif))

erad %>% dplyr::select(-geotex, -weedsp_geotex, -longitude, -degradation) -> erad

rm(modvif, mydata)
```


    
#### - Distributions and dispersion parameters
##### - Looking for outliers

We will look for potential outliers in the data using **boxplots** and **Cleveland dotplots** on our quantitative variables. 

```{r erad boxplot outliers, fig.align='center', fig.cap="**Figure 3.** Boxplots for all quantitative variables of the *eradication efficiency* dataset", warning=FALSE}
### IMPORTANT NOTE: as ggplot2 requires data in the long format to be able to do multi-panels plots (known as facets), we need to transform our data using the pivot_longer() function of {tidyr}. 
# To keep only numeric columns: num.erad <- erad[, sapply(erad, is.numeric)]; or we can do:

# With ggplot2, it is time-consuming!
#erad %>%
  #purrr::keep(is.numeric) %>% # Keep only numeric columns
  #tidyr::pivot_longer(cols = tidyselect::everything()) %>% # Pivot every columns
  #ggplot2::ggplot(ggplot2::aes(x = name, y = value)) +
  #ggplot2::geom_boxplot(fill = "lightsalmon") +
  #ggplot2::theme_minimal() -> ppp
#ppp + ggplot2::facet_wrap( ~ name, scales = "free")

#RColorBrewer::display.brewer.all() To display all the colors in {RColorBrewer}

jk.dusz.tarping::uni.boxplots(dataset = erad)
```

```{r erad dotplot, fig.align='center', fig.cap="**Figure 4.** Cleveland dotplots for all quantitative variables of the *eradication efficiency* dataset"}
jk.dusz.tarping::uni.dotplots(dataset = erad)
```

By the look of these graphs, it is clear that we have many *extreme values* (possible outliers). Yet, extreme values are not necessarily **true outliers**:

* The variables *latitude*, *slope* and *coarse_env*  look fine.
* The variables *efficiency*, *obstacles*, *flood*, *distance*, *tarping_duration*, and *trench_depth* may require mild transformations to be normalized but do not seem to contain too extreme values.
* The variables *elevation*, *freq_monitoring*, and *sedicover_height* may require important transformations to be normalized. It would be interesting to see if their extreme values are too far from what could be expected from a Normal or a Poisson distribution (see below).
* The *stand_surface* variable contains a problematic patterns/outliers.


```{r erad simu.distrib, include=FALSE, eval=FALSE}
simu.var <- dplyr::select(.data = erad, elevation, freq_monitoring, stand_surface, distance, sedicover_height, trench_depth, tarping_duration)

jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "log-normal")
jk.dusz.tarping::uni.simudistrib(simu.var = simu.var, distribution = "poisson")
rm(simu.var)

```

After analysing random samples drawn from log-normal distributions (based on the parameters of our potentially problematic variables), we have good reason to believe that only *stand_surface* should be problematic. Consequently, we choose to delete the spotted outlier within *stand_surface* (i.e. the *stand_surface* of ca. 5000 m^2^).    
The other variables do not have distribution that a transformation could not fix. We will see later on whether we actually need such transformation (e.g log) or not. 


```{r erad update, include=FALSE}
erad <- erad[-(which.max(erad$stand_surface)),]
```


   
##### - Normality

We already got a pretty good idea of the distribution of our variables and not all statistical methods involve a Normality assumption (for instance, multiple linear regressions assume that residuals are normally distributed, not the variable themselves). Still, it's interesting and often useful to have a precise view of variables distributions, even for predictors and covariates. 

```{r erad histo, fig.align='center', fig.cap="**Figure 5.** Histograms for all quantitative variables of the *eradication efficiency* dataset"}
uni.histograms(dataset = erad)
```





#### - Bivariate relationships
##### - Relationships among variables

The next step in our data exploration is to plot the response variables against each predictors and covariates in order to assess the **linearity** and **homoscedasticity** assumptions as well as identify potential **interactions** and **multivariate outliers**. It would be interesting to plot all variables against each other, but we have too many potential predictors and covariates for that.

```{r erad bivariate plot, fig.align='center', fig.cap="**Figure 6.** Multivariate representation of the *efficiency score* against each potential explanatory variable with a focus on whether knotweed stands were fully tarped or not", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
### To plot all variables against each other, we can use pairs() or ggpairs():
# ggplot2::theme_set(ggplot2::theme_bw())
# erad %>% dplyr::select(-xp_id, -goals) %>% GGally::ggpairs()

### To plot a response variable against all predictors/covariates, we will have to do use a for loop or the facet_wrap function of ggplot (or equivalent).
# For the "efficiency" variable (bounded continuous variable):
erad %>% dplyr::select(-xp_id, -goals, -eff_eradication, -high_eff) %>%
  dplyr::mutate_if(is.factor, as.numfactor) %>% 
  tidyr::pivot_longer(-c(efficiency, fully_tarped), # Means that all columns of `mydata` need to be reshaped except "efficiency" & "fully_tarped" (works as well with the form `!efficiency`, and we can place it somewhere else: actually, it is the `cols =` argument)! We have to work in the long-format because that's how the tidyverser and thus ggplot2 love to work...
                               names_to = "Predictor", # Gives the name of the new grouping variable while "values_to" gives the name of the variable that will be created from the data stored in the cell value.
                               values_to = "Value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = Value, y = efficiency)) +
  ggplot2::facet_wrap(~Predictor, scales = "free_x", strip.position = "bottom") +
  ggplot2::geom_point(size = 1.5, alpha = 0.8, ggplot2::aes(col = as.factor(fully_tarped))) +
  ggplot2::scale_y_continuous(limits=c(0,10)) +
  ggplot2::geom_smooth(method = "lm", na.rm = TRUE, size = 1, 
                       ggplot2::aes(col = as.factor(fully_tarped))) + # If you want to affect something related to the input dataframe (such as here, where we want a smoother for each value of "fully_tarped"), you have to put the command in an aes() function!
# If a "loess" smoother is used, sometimes the smoother line won't be plotted, I don't know why.
  ggplot2::scale_colour_manual(values = c("plum1", "palevioletred4")) +
  ggplot2::labs(x = "Predictor value", y = "Efficiency score", col = "Fully tarped") +
  ggthemes::theme_hc(style = "darkunica") +
  ggplot2::theme(panel.border = ggplot2::element_blank(),
          legend.key = ggplot2::element_blank(),
          strip.background = ggplot2::element_blank(),
          strip.text = ggplot2::element_text(colour = "gray60", size = 10),
          strip.placement = "outside",
          axis.line.x = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          axis.line.y = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          legend.position = "right")
```

Various interesting observations can be made from these scatterplots:

* First, heteroscedasticity will perhaps not be surprising in our models as the variances of many variables do not seem to be homogeneous. On the other hand, we do not see clear multivariate outliers.
* Second, the *efficiency* score seems to increase with increasing *distance*. This may be evidence in support to one of our working hypotheses stating that successful tarping operations cover areas well beyond the visible limits of knotweed stands. Conversely, the *efficiency* score seems to decrease with increasing presence of *obstacles*.
* Third, surprisingly, the *efficiency* score also seems to increase with increasing *elevation*. If this is not due to measurement error, then it could be a sign of stronger abiotic constraints on knotweeds.
* Fourth, there may be signs of interesting interactions between the *fully tarped* variable and *slope*, *coarse_env*, *stand_surface*, *tarping_duration*, *trench_depth* and *uprootexcav*, although caution is required as variances are quite high. Furthermore, if the interaction with *stand_surface* is relatively straightforward, the other ones are trickier do grasp and could simply be noise.

To further investigate multivariate relationship, we will create another serie of scatterplots (but we do not intend on investigating all possible combinations).

```{r erad bivariate plot2, fig.align='center', fig.cap="**Figure 6bis.** Multivariate representation of the *efficiency score* against each potential explanatory variable with a focus on whether knotweed stands were fully tarped or not", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
erad %>% dplyr::select(-xp_id, -goals, -eff_eradication, -high_eff) %>%
  dplyr::mutate_if(is.factor, as.numfactor) %>% 
  tidyr::pivot_longer(-c(efficiency, plantation), 
                               names_to = "Predictor", 
                               values_to = "Value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = Value, y = efficiency)) +
  ggplot2::facet_wrap(~Predictor, scales = "free_x", strip.position = "bottom") +
  ggplot2::geom_point(size = 1.5, alpha = 0.8, ggplot2::aes(col = as.factor(plantation))) +
  ggplot2::scale_y_continuous(limits=c(0,10.5)) +
  ggplot2::geom_smooth(method = "lm", na.rm = TRUE, size = 1, 
                       ggplot2::aes(col = as.factor(plantation))) + 
  ggplot2::scale_colour_manual(values = c("goldenrod3", "coral", "darkred")) +
  ggplot2::labs(x = "Predictor value", y = "Efficiency score", col = "Plantation") +
  ggthemes::theme_hc(style = "darkunica") +
  ggplot2::theme(panel.border = ggplot2::element_blank(),
          legend.key = ggplot2::element_blank(),
          strip.background = ggplot2::element_blank(),
          strip.text = ggplot2::element_text(colour = "gray60", size = 10),
          strip.placement = "outside",
          axis.line.x = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          axis.line.y = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          legend.position = "right")
```

We can see from this new panel of plots that the *efficiency* score seems to increase when knotweeds are *fully tarped*, as we hypothesized. 


## Next steps for preparation

0) New IDs for random effects????
1) Multiple imputation (Mice or Amelia)
2) Transformations and standardization (for scaling, especially important when I use interactions terms; cf. "standardization" post on CV in my MacBook favorites)
3) Deal with *tarping_duration*. Include weights?

